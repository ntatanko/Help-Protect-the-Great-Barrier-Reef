{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7731498-c309-4770-89d9-23fdd3a9ffb0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590a09d2-5ebe-4b56-9225-fd06eb02ecb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DF_PART = \"/app/_data/tensorflow-great-barrier-reef/train.csv\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "LABEL_FOLDER = \"labels\"\n",
    "SEED = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01e1b5f1-c73b-4d0e-a6e7-839746321377",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/_data/sequences.json\", \"r\") as f:\n",
    "    seq_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a5527c-0d42-4e7a-ac9f-3db1cdc3c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DF_PART)\n",
    "df[\"img_path\"] = (\n",
    "    \"/app/_data/tensorflow-great-barrier-reef/train_images/video_\"\n",
    "    + df.video_id.astype(\"str\")\n",
    "    + \"/\"\n",
    "    + df.video_frame.astype(\"str\")\n",
    "    + \".jpg\"\n",
    ")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"len_annotation\"] = df[\"annotations\"].str.len()\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"-\", \"_\", regex=True)\n",
    "df[\"new_img_path\"] = f\"/app/_data/{IMAGE_FOLDER}/\" + df[\"image_id\"] + \".jpg\"\n",
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "R = df[df[\"len_annotation\"] == 0].shape[0] / df[df[\"len_annotation\"] != 0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd1afac-afd8-4724-896e-367b271e2e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sequence\"].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd3fe5-aad8-40a1-836f-1b7dafa2dc3f",
   "metadata": {},
   "source": [
    "## KFold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eed7ca1-151f-4ada-ae95-a93fef98e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa7ebff9-0cff-45df-9fe9-883b160a6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids = '3'\n",
    "# train_sequences = seq_dict[ids]['train']\n",
    "# val_sequences = seq_dict[ids]['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02642ad7-8c42-4018-804f-dd5e59572abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"sequence in @train_sequences and len_annotation!=0\"),\n",
    "#         df.query(\"sequence in @train_sequences and len_annotation==0\").sample(\n",
    "#             int(\n",
    "#                 df.query(\"sequence in @train_sequences and len_annotation!=0\").shape[0]\n",
    "#                 * 0.07\n",
    "#             )\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac = 1)\n",
    "# val = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"sequence in @val_sequences and len_annotation!=0\"),\n",
    "#         df.query(\"sequence in @val_sequences and len_annotation==0\").sample(\n",
    "#             int(\n",
    "#                 df.query(\"sequence in @val_sequences and len_annotation!=0\").shape[0]\n",
    "#                 * R\n",
    "#             )\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cc78ef3-5b6d-4431-b4c4-577f5fed04a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_ID = 2\n",
    "train = pd.concat(\n",
    "    [\n",
    "        df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\"),\n",
    "        df.query(\"video_id!=@VIDEO_ID and len_annotation==0\").sample(\n",
    "            int(df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\").shape[0] * 0.03)\n",
    "        ),\n",
    "    ]\n",
    ").sample(frac=1)\n",
    "val = df.query(\"video_id==@VIDEO_ID\").sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da5c15d7-bb9d-43b7-b7df-93a31c2c3292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    9449\n",
       "label             4242\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47cd3774-ac8c-4d1b-9cde-7e8501934255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    2449\n",
       "label              677\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e20b719-f110-4b42-b5cf-0e5bf6802fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    3.858310\n",
       "label             6.265879\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a572d2f-2c88-4a61-beb6-d04d06b1af9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4369, 8561)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids = train.index.tolist()\n",
    "val_ids = val.index.tolist()\n",
    "\n",
    "len(train_ids), len(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c71f1ba3-6610-441c-bb82-c5185a9c09a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5_f2/data/reef_data_val2.yaml'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/app/_data/val_37_val2.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt = f\"/app/_data/train_{SEED}_val{VIDEO_ID}.txt\"\n",
    "val_txt = train_txt.replace(\"train_\", \"val_\")\n",
    "data_yaml_path = f\"/app/_data/yolov5_f2/data/reef_data_val{VIDEO_ID}.yaml\"\n",
    "data_yaml_path\n",
    "val_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a7099a-e3bb-460f-b02a-f07372135162",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = df.loc[train_ids, \"new_img_path\"].tolist()\n",
    "val_img_path = df.loc[val_ids, \"new_img_path\"].tolist()\n",
    "np.savetxt(\n",
    "    train_txt,\n",
    "    train_img_path,\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "np.savetxt(val_txt, val_img_path, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda8c343-47e5-40e0-b894-70d03f3f1613",
   "metadata": {},
   "source": [
    "## Custimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8e8972e-5ff5-4c36-b856-2bcabf70862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b76c8913-18c5-408d-8d6a-120d341d5efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate {data_yaml_path}\n",
    "\n",
    "train: {train_txt}  # training directory\n",
    "val: {val_txt}  # validation directory\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: [\"starfish\"]  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbd23708-c95e-4ee5-ad99-76961057f493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: /app/_data/train_37_val2.txt # training directory\n",
      "val: /app/_data/val_37_val2.txt # validation directory\n",
      "\n",
      "# Classes\n",
      "nc: 1  # number of classes\n",
      "names: ['starfish']  # class names\n"
     ]
    }
   ],
   "source": [
    "!cat {data_yaml_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1dc1589c-984e-4262-9598-ae689e3b5fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
      "# Hyperparameters for COCO training from scratch\n",
      "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
      "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
      "\n",
      "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
      "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
      "momentum: 0.937  # SGD momentum/Adam beta1\n",
      "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
      "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
      "warmup_momentum: 0.8  # warmup initial momentum\n",
      "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
      "box: 0.05  # box loss gain\n",
      "cls: 0.5  # cls loss gain\n",
      "cls_pw: 1.0  # cls BCELoss positive_weight\n",
      "obj: 1.0  # obj loss gain (scale with pixels)\n",
      "obj_pw: 1.0  # obj BCELoss positive_weight\n",
      "iou_t: 0.20  # IoU training threshold\n",
      "anchor_t: 4.0  # anchor-multiple threshold\n",
      "# anchors: 3  # anchors per output layer (0 to ignore)\n",
      "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
      "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
      "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
      "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
      "degrees: 0.0  # image rotation (+/- deg)\n",
      "translate: 0.1  # image translation (+/- fraction)\n",
      "scale: 0.5  # image scale (+/- gain)\n",
      "shear: 0.0  # image shear (+/- deg)\n",
      "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
      "flipud: 0.0  # image flip up-down (probability)\n",
      "fliplr: 0.5  # image flip left-right (probability)\n",
      "mosaic: 1.0  # image mosaic (probability)\n",
      "mixup: 0.0  # image mixup (probability)\n",
      "copy_paste: 0.0  # segment copy-paste (probability)\n"
     ]
    }
   ],
   "source": [
    "!cat /app/_data/yolov5_f2/data/hyps/hyp.scratch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f3e57f3-07b1-4970-aaa4-b8295a093027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate /app/_data/yolov5_f2/data/hyps/hyp.custom.val2.yaml\n",
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "lr0: 0.005  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.4  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
    "degrees: 1.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.1  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.1  # image flip up-down (probability)\n",
    "fliplr: 0.6  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bd5064-8d51-4bd7-96cb-dcf18aebbf5f",
   "metadata": {},
   "source": [
    "# yolov5 requirements and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94a91f8d-e5f4-4985-86e7-2530d9ccf40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb\n",
    "clear_output()\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc8d8468-df12-47c1-8d4a-80dcd1e82503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app/_data/yolov5_f2/\n",
    "!pip install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b36576a-ec77-43d7-935a-5c13b9338abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in glob.glob(\"/app/_data/yolov5_f2/runs/train/*\"):\n",
    "#     if 'NAME' in path:\n",
    "#         path\n",
    "#         shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadaab39-8a46-4b5e-8c72-fc0ae5d1609a",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f4cc7b3-81ba-46d5-ba03-be2c7ca9fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5l6_2560_val2_f2'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS = \"yolov5l6.pt\"\n",
    "IMG_SIZE = 1280 * 2\n",
    "NAME = f\"{WEIGHTS[:-3]}_{IMG_SIZE}_val{VIDEO_ID}_f2\"\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd61aa6e-c4fd-49a3-a1a7-36e51af1219e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5l6_2560_val2_f2'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98faf64f-910d-430d-a690-895ce39e4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in glob.glob(f\"/app/_data/yolov5_f2/*/*/{NAME}*\"):\n",
    "    shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73e70545-0c79-44f9-8514-fb7c14c79de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l6.pt, cfg=, data=/app/_data/yolov5_f2/data/reef_data_val2.yaml, hyp=data/hyps/hyp.custom.val2.yaml, epochs=80, batch_size=4, imgsz=2560, rect=True, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=True, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=yolov5l6_2560_val2_f2, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=10, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24265MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.005, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.4, hsv_v=0.4, degrees=1.0, translate=0.1, scale=0.1, shear=0.0, perspective=0.0, flipud=0.1, fliplr=0.6, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myolov5l6_2560_val2_f2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5/runs/hinh9jpk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /app/_data/yolov5_f2/wandb/run-20220203_191959-hinh9jpk\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   3540480  models.common.Conv                      [512, 768, 3, 2]              \n",
      "  8                -1  3   5611008  models.common.C3                        [768, 768, 3]                 \n",
      "  9                -1  1   7079936  models.common.Conv                      [768, 1024, 3, 2]             \n",
      " 10                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      " 11                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 12                -1  1    787968  models.common.Conv                      [1024, 768, 1, 1]             \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  3   6200832  models.common.C3                        [1536, 768, 3, False]         \n",
      " 16                -1  1    394240  models.common.Conv                      [768, 512, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 24                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 27                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  3   5807616  models.common.C3                        [1024, 768, 3, False]         \n",
      " 30                -1  1   5309952  models.common.Conv                      [768, 768, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  3  10496000  models.common.C3                        [1536, 1024, 3, False]        \n",
      " 33  [23, 26, 29, 32]  1     46152  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [256, 512, 768, 1024]]\n",
      "Model Summary: 607 layers, 76162504 parameters, 76162504 gradients, 110.2 GFLOPs\n",
      "\n",
      "Transferred 787/795 items from yolov5l6.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight (no decay), 135 weight, 135 bias\n",
      "WARNING: --rect is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 3)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 3)), ToGray(always_apply=False, p=0.01), RandomBrightnessContrast(always_apply=False, p=0.1, brightness_limit=(-0.05, 0.05), contrast_limit=(-0.05, 0.05), brightness_by_max=True), RandomGamma(always_apply=False, p=0.05, gamma_limit=(80, 120), eps=None), ImageCompression(always_apply=False, p=0.05, quality_lower=95, quality_upper=100, compression_type=0), Sharpen(always_apply=False, p=0.05, alpha=(0.2, 0.3), lightness=(0.5, 1.0)), HueSaturationValue(always_apply=False, p=0.01, hue_shift_limit=(-5, 5), sat_shift_limit=(-5, 5), val_shift_limit=(-5, 5)), RGBShift(always_apply=False, p=0.01, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), Flip(always_apply=False, p=0.01), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit_x=(-0.2, 0.2), shift_limit_y=(-0.2, 0.2), scale_limit=(0.10000000000000009, 0.19999999999999996), rotate_limit=(10, 45), interpolation=1, border_mode=4, value=None, mask_value=None), RandomRain(always_apply=False, p=0.01, slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.6, rain_type='drizzle')\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/app/_data/train_37_val2' images and labels...4242 found, 127 m\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /app/_data/train_37_val2.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/app/_data/val_37_val2.cache' images and labels... 677 found, 788\u001b[0m\n",
      "Plotting labels to runs/train/yolov5l6_2560_val2_f2/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.48 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Image sizes 2560 train, 2560 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov5l6_2560_val2_f2\u001b[0m\n",
      "Starting training for 80 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/79     21.3G    0.0638   0.05871         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449   0.000334     0.0102   0.000162   4.16e-05\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/79     20.5G   0.04947    0.0438         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.352      0.476        0.4      0.297\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/79     20.5G   0.04272   0.03652         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.585      0.635      0.612      0.423\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/79     20.5G   0.03899   0.03084         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.498      0.649      0.616      0.439\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/79     20.5G   0.03566   0.02748         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.553      0.661      0.644      0.407\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/79     20.5G   0.03362    0.0256         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.605      0.697       0.71      0.538\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/79     20.5G   0.03111   0.02393         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.785      0.727      0.768      0.578\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/79     20.5G   0.02984   0.02232         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.668      0.734      0.756      0.593\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/79     20.5G   0.02905   0.02142         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.732      0.746      0.784      0.623\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/79     20.5G   0.02713   0.02047         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.725      0.801      0.823       0.65\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/79     20.5G   0.02663   0.02001         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.772      0.759      0.798      0.625\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/79     20.5G   0.02611   0.01958         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.726       0.73      0.764      0.617\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/79     20.5G   0.02565   0.01918         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.651      0.775      0.788      0.629\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/79     20.5G   0.02463   0.01863         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449       0.79      0.701      0.754      0.597\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/79     20.5G   0.02425   0.01844         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.762      0.717      0.766      0.611\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/79     20.5G    0.0242   0.01823         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.753      0.797      0.831      0.671\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/79     20.5G   0.02267   0.01763         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449       0.72      0.701      0.733       0.58\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/79     20.5G   0.02266   0.01724         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.733      0.741      0.777      0.595\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/79     20.5G   0.02205   0.01724         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.736      0.779      0.801      0.619\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/79     20.5G   0.02217    0.0169         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.702      0.764      0.785      0.613\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/79     20.5G   0.02205   0.01696         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.756       0.72      0.761      0.607\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/79     20.5G   0.02178   0.01699         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.837       0.78      0.825      0.664\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/79     20.5G   0.02148   0.01628         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.794      0.748      0.789      0.621\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/79     20.5G   0.02094   0.01644         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.778      0.786      0.823      0.661\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/79     20.5G   0.02048   0.01623         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.756      0.755      0.789      0.625\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/79     20.5G   0.02065   0.01602         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.786      0.771      0.805      0.652\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     26/79     20.5G   0.02035   0.01587         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.785      0.757      0.794      0.625\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     27/79     20.5G   0.01973    0.0157         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.758      0.724      0.763      0.618\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     28/79     20.5G   0.02003   0.01568         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.775      0.773      0.812      0.658\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     29/79     20.5G   0.01931    0.0152         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.724      0.765       0.79      0.638\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     30/79     20.5G   0.01928   0.01535         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.754      0.739       0.78      0.632\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     31/79     20.5G   0.01932    0.0151         0         0      2560: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.839      0.704      0.756      0.613\n",
      "Stopping training early as no improvement observed in last 10 epochs. Best results observed at epoch 21, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n",
      "\n",
      "32 epochs completed in 16.804 hours.\n",
      "Optimizer stripped from runs/train/yolov5l6_2560_val2_f2/weights/last.pt, 154.8MB\n",
      "Optimizer stripped from runs/train/yolov5l6_2560_val2_f2/weights/best.pt, 154.8MB\n",
      "\n",
      "Validating runs/train/yolov5l6_2560_val2_f2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       8561       2449      0.843      0.778      0.825      0.664\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1770... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             metrics/F2 â–â–…â–‡â–†â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 â–â–„â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 â–â–„â–…â–†â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision â–â–„â–†â–…â–†â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall â–â–…â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–ˆâ–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss â–ˆâ–†â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss â–ˆâ–†â–„â–„â–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss â–ˆâ–„â–„â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–‚â–â–â–â–â–â–‚â–‚â–‚â–â–‚â–â–‚â–â–‚â–â–‚â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss â–ˆâ–„â–ƒâ–„â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–‚â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–†â–†â–…â–…â–…\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                best/F2 0.79041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             best/epoch 21\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best/mAP_0.5 0.82523\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      best/mAP_0.5:0.95 0.66378\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         best/precision 0.8373\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            best/recall 0.7795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             metrics/F2 0.79032\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.82512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.66384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.84261\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.77824\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.01932\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.0151\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.00282\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.00365\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00361\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 144 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33myolov5l6_2560_val2_f2\u001b[0m: \u001b[34mhttps://wandb.ai/tatanko/YOLOv5/runs/hinh9jpk\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220203_191959-hinh9jpk/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "Results saved to \u001b[1mruns/train/yolov5l6_2560_val2_f2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img {IMG_SIZE} \\\n",
    "                --batch 4\\\n",
    "                --epochs 80 \\\n",
    "                --data {data_yaml_path} \\\n",
    "                --weights {WEIGHTS} \\\n",
    "                --name {NAME} \\\n",
    "                --hyp data/hyps/hyp.custom.val2.yaml \\\n",
    "                --single-cls \\\n",
    "                --patience 10 \\\n",
    "                --rect \\\n",
    "                --workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e7c53b3-d455-4fcf-ba48-2c5dfc5a64f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = (\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "    )\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, \"int16\")\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n] = tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, \"int16\")\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18ba4354-88f2-4477-a036-54fb80c08d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"r\") as f:\n",
    "    res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cbf2b66c-91dc-4e7c-80c7-83defaf944ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24265MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 378 layers, 35248920 parameters, 0 gradients, 49.0 GFLOPs\n",
      "Adding AutoShape... \n",
      "100% 8561/8561 [18:37<00:00,  7.66it/s]\n"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "# path = f\"/app/_data/yolov5_f2/runs/train/{NAME}/weights/best.pt\"\n",
    "path = '/app/_data/yolov5_f2/runs/train/yolov5m6_2560_val2_f2/weights/best.pt'\n",
    "\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "# chose validation set\n",
    "df_test = val.copy()\n",
    "# computing f2 score\n",
    "for ix in tqdm(df_test.index.tolist()):\n",
    "    img = np.array(Image.open(df_test.loc[ix, \"img_path\"]))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    prediction = prediction[prediction[:, 4] > 0.1]\n",
    "    gt = np.array([list(x.values()) for x in df_test.loc[ix, \"annotations\"]])\n",
    "    if gt.size:\n",
    "        gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "        gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "    for n, c_th in enumerate(conf_thres):\n",
    "        TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "        res[n, 0, :] += TP\n",
    "        res[n, 1, :] += FP\n",
    "        res[n, 2, :] += FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ff930b3-bae2-42be-84bc-3ee104e7484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)\n",
    "if path not in res_dict:\n",
    "    res_dict[path] = {\n",
    "        IMG_SIZE: {\n",
    "            \"best\": [\n",
    "                np.round(conf_thres[np.argmax(F2)], 2),\n",
    "                np.round(np.max(F2), 4),\n",
    "            ],\n",
    "            \"all\": list(np.round(F2, 4)),\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    res_dict[path][IMG_SIZE] = {\n",
    "        \"best\": [\n",
    "            np.round(conf_thres[np.argmax(F2)], 2),\n",
    "            np.round(np.max(F2), 4),\n",
    "        ],\n",
    "        \"all\": list(np.round(F2, 4)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "34173ee8-526a-409f-a3f5-fc9a6b36bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': [0.23, 0.659],\n",
       " 'all': [0.6293,\n",
       "  0.6344,\n",
       "  0.6389,\n",
       "  0.643,\n",
       "  0.6462,\n",
       "  0.6486,\n",
       "  0.6517,\n",
       "  0.6531,\n",
       "  0.654,\n",
       "  0.6545,\n",
       "  0.6557,\n",
       "  0.6576,\n",
       "  0.6583,\n",
       "  0.659,\n",
       "  0.6586,\n",
       "  0.659,\n",
       "  0.6586,\n",
       "  0.658,\n",
       "  0.6579,\n",
       "  0.6567,\n",
       "  0.6559,\n",
       "  0.6556,\n",
       "  0.6554,\n",
       "  0.6553,\n",
       "  0.6551,\n",
       "  0.6531,\n",
       "  0.6528,\n",
       "  0.6526,\n",
       "  0.6516,\n",
       "  0.6496,\n",
       "  0.6475,\n",
       "  0.6463,\n",
       "  0.6464,\n",
       "  0.6456,\n",
       "  0.6432,\n",
       "  0.6401,\n",
       "  0.6399,\n",
       "  0.6363,\n",
       "  0.635,\n",
       "  0.6342,\n",
       "  0.6295,\n",
       "  0.627,\n",
       "  0.6259,\n",
       "  0.6247,\n",
       "  0.6217,\n",
       "  0.6191,\n",
       "  0.6166,\n",
       "  0.6133,\n",
       "  0.6103,\n",
       "  0.6088,\n",
       "  0.6058]}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict[path][IMG_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d833898-3dc0-4411-991d-31b05c0ac228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img, bbox, pred=None):\n",
    "    img_h, img_w = img.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "    ax.imshow(img)\n",
    "    for i in range(len(bbox)):\n",
    "        x_c, y_c, w, h = bbox[i]\n",
    "        rect = plt.Rectangle(\n",
    "            [(x_c - w / 2) * img_w, (y_c - h / 2) * img_h],\n",
    "            w * img_w,\n",
    "            h * img_h,\n",
    "            ec=\"b\",\n",
    "            fc=\"none\",\n",
    "            lw=2.0,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    if pred is not None:\n",
    "        for i in range(len(pred)):\n",
    "            x_c, y_c, w, h = pred[i]\n",
    "            rect = plt.Rectangle(\n",
    "                [x_c - w / 2, y_c - h / 2],\n",
    "                w,\n",
    "                h,\n",
    "                ec=\"r\",\n",
    "                fc=\"none\",\n",
    "                lw=2.0,\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "297ee4d8-5cd5-4fd5-8632-616f6d3ec1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8436198c-764e-4145-9fac-c3fabd6f6ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "#         A.ShiftScaleRotate(shift_limit=0.05, scale_limit=[0.05, 0.1], rotate_limit=10, border_mode=0, value=(114, 114, 114),p=1.0),\n",
    "        A.HueSaturationValue(\n",
    "            hue_shift_limit=3, sat_shift_limit=3, val_shift_limit=5, p=1\n",
    "        ),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.01, contrast_limit=0.05, brightness_by_max=True, p=1\n",
    "        ),\n",
    "        A.Flip(p=1)\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"yolo\", min_visibility=0.4, label_fields=[\"class_labels\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8aa30942-486a-4710-abd2-e7a754c13839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24265MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "path = f\"/app/_data/yolov5_f2/runs/train/{NAME}/weights/best.pt\"\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "df_test = val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "201c2796-f3dc-4f8c-a2e1-1bf0dac9c236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.concat([val.query(\"len_annotation>0\"),\n",
    "           val.query(\"len_annotation==0\").sample(1000)]).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66934a4f-930c-4e65-84d2-b0f339de745f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2560': {'best': [0.23, 0.659],\n",
       "  'all': [0.6293,\n",
       "   0.6344,\n",
       "   0.6389,\n",
       "   0.643,\n",
       "   0.6462,\n",
       "   0.6486,\n",
       "   0.6517,\n",
       "   0.6531,\n",
       "   0.654,\n",
       "   0.6545,\n",
       "   0.6557,\n",
       "   0.6576,\n",
       "   0.6583,\n",
       "   0.659,\n",
       "   0.6586,\n",
       "   0.659,\n",
       "   0.6586,\n",
       "   0.658,\n",
       "   0.6579,\n",
       "   0.6567,\n",
       "   0.6559,\n",
       "   0.6556,\n",
       "   0.6554,\n",
       "   0.6553,\n",
       "   0.6551,\n",
       "   0.6531,\n",
       "   0.6528,\n",
       "   0.6526,\n",
       "   0.6516,\n",
       "   0.6496,\n",
       "   0.6475,\n",
       "   0.6463,\n",
       "   0.6464,\n",
       "   0.6456,\n",
       "   0.6432,\n",
       "   0.6401,\n",
       "   0.6399,\n",
       "   0.6363,\n",
       "   0.635,\n",
       "   0.6342,\n",
       "   0.6295,\n",
       "   0.627,\n",
       "   0.6259,\n",
       "   0.6247,\n",
       "   0.6217,\n",
       "   0.6191,\n",
       "   0.6166,\n",
       "   0.6133,\n",
       "   0.6103,\n",
       "   0.6088,\n",
       "   0.6058]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['/app/_data/yolov5_f2/runs/train/yolov5m6_2560_val2_f2/weights/best.pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46af8d3-7ece-4137-bb2f-6a51a586310b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# img_w = 1280\n",
    "# img_h = 720\n",
    "# for ix in tqdm(df_test.index):\n",
    "#     image_path = df_test.loc[ix, \"img_path\"]\n",
    "#     img_name = df_test.loc[ix, \"image_id\"]\n",
    "#     annotations = df_test.loc[ix, \"annotations\"]\n",
    "#     img = np.array(Image.open(image_path))\n",
    "#     bboxes = np.zeros([len(annotations), 5])\n",
    "#     for i in range(len(annotations)):\n",
    "#         xmin = annotations[i][\"x\"] / img_w\n",
    "#         ymin = annotations[i][\"y\"] / img_h\n",
    "#         width = annotations[i][\"width\"] / img_w\n",
    "#         height = annotations[i][\"height\"] / img_h\n",
    "#         width = width if (width + xmin) <= 1 else (1 - xmin)\n",
    "#         height = height if (height + ymin) <= 1 else (1 - ymin)\n",
    "#         x_center = xmin + width / 2\n",
    "#         y_center = ymin + height / 2\n",
    "#         bboxes[i:, 0] = 0\n",
    "#         bboxes[i:, 1] = x_center\n",
    "#         bboxes[i:, 2] = y_center\n",
    "#         bboxes[i:, 3] = width\n",
    "#         bboxes[i:, 4] = height\n",
    "#     for n in range(5):\n",
    "#         transformed = transform(\n",
    "#             image=img,\n",
    "#             bboxes=bboxes[:, 1:],\n",
    "#             class_labels=bboxes[:, 0],\n",
    "#         )\n",
    "#         a_img, a_bbox = transformed[\"image\"], transformed[\"bboxes\"]\n",
    "#         prediction = model(a_img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "#         prediction = prediction[prediction[:, 4] > 0.1]\n",
    "#         gt = np.array(a_bbox)\n",
    "#         if gt.size:\n",
    "#             gt[:, 0] *= 1280\n",
    "#             gt[:, 1] *= 720\n",
    "#             gt[:, 2] *= 1280\n",
    "#             gt[:, 3] *= 720\n",
    "#             gt[:, 0] = gt[:, 0] - gt[:, 2] / 2\n",
    "#             gt[:, 1] = gt[:, 1] - gt[:, 3] / 2\n",
    "#             gt[:, 2] = gt[:, 0] + gt[:, 2]\n",
    "#             gt[:, 3] = gt[:, 1] + gt[:, 3]\n",
    "\n",
    "#         for n, c_th in enumerate(conf_thres):\n",
    "#             TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "#             res[n, 0, :] += TP\n",
    "#             res[n, 1, :] += FP\n",
    "#             res[n, 2, :] += FN\n",
    "#         show_img(a_img, a_bbox, prediction[:, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "96d302d2-6367-40f2-bb8e-3d96f8fce2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 6), dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7a60130a-5dc5-43a5-972a-02bff869ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best 0.18 0.6667 \n",
      "all [0.6612, 0.6612, 0.6612, 0.6612, 0.6612, 0.6612, 0.6639, 0.6639, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667, 0.6667]\n"
     ]
    }
   ],
   "source": [
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)\n",
    "print(\"best\", np.round(conf_thres[np.argmax(F2)], 2), np.round(np.max(F2), 4), \"\\nall\", list(np.round(F2, 4)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "35f9dfad-94a5-4c76-a894-069de1b06d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5_f2/runs/train/yolov5l6_2560_val2_f2/weights/best.pt'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/app/_data/yolov5_f2/runs/train/yolov5l6_2560_val2_f2/weights/best.pt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bcd48b5c-9f80-41eb-80fe-1686d0e64ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/YOLOv5full_train_weights/yolov5m6_2560_val2_f2.pt'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(path, f'/app/_data/YOLOv5full_train_weights/yolov5m6_2560_val2_f2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9cdab282-000d-42a9-bef1-df002494a591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5l6_2560_val2_f2'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9209d69f-c491-49f2-88bc-2aa546971114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
