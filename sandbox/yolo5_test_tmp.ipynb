{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cac4b4-ebe3-4112-901e-60878d9c23bd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold, KFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import copy\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b553b52-bc65-4e14-8f8c-9dc641c4197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_paths = [\n",
    "    \"/app/_data/yolov5/runs/train/yolov5l6_val2/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5l6_val110/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val23/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val1/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val3_3840/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5s6_val3_4480/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val02/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val3_3840/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val02/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val23/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val2/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5s6_val29/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val1/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5l6_val110/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m6_val2/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5s6_val29/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5s6_val3_4480/weights/last.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/best.pt\",\n",
    "    \"/app/_data/yolov5/runs/train/yolov5l6_val2/weights/last.pt\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7781c47-ff4c-4b83-b33b-2587464daf9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF_PART = \"/app/_data/tensorflow-great-barrier-reef/train.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE\n",
    "SEED = 42\n",
    "IMAGE_FOLDER = \"norm_images/images\"\n",
    "LABEL_FOLDER = IMAGE_FOLDER.replace(\"/images\", \"/labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f289d42c-c940-4525-b5f8-5829bd2b77e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU_coco(bbox1, bbox2):\n",
    "    x_left = max(bbox1[0], bbox2[0])\n",
    "    y_top = max(bbox1[1], bbox2[1])\n",
    "    x_right = min(bbox1[0] + bbox1[2], bbox2[0] + bbox2[2])\n",
    "    y_bottom = min(bbox1[1] + bbox1[3], bbox2[1] + bbox2[3])\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    bb1_area = bbox1[2] * bbox1[3]\n",
    "    bb2_area = bbox2[2] * bbox2[3]\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou\n",
    "\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff15ee7-cb50-45e5-8241-3c2f60506c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DF_PART)\n",
    "df[\"img_path\"] = (\n",
    "    \"/app/_data/tensorflow-great-barrier-reef/train_images/video_\"\n",
    "    + df.video_id.astype(\"str\")\n",
    "    + \"/\"\n",
    "    + df.video_frame.astype(\"str\")\n",
    "    + \".jpg\"\n",
    ")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"len_annotation\"] = df[\"annotations\"].str.len()\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"-\", \"_\", regex=True)\n",
    "df[\"new_img_path\"] = f\"/app/_data/{IMAGE_FOLDER}/\" + df[\"image_id\"] + \".jpg\"\n",
    "\n",
    "\n",
    "df[\"num_bbox\"] = df[\"len_annotation\"]\n",
    "df[\"bboxes\"] = df.annotations.apply(get_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cb262fe-ec0e-468f-b4a2-5f260b3695d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "video_id\n",
       "0    3065\n",
       "1    6384\n",
       "2    2449\n",
       "Name: len_annotation, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"video_id\")[\"len_annotation\"].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fce7f9-31a8-4ee8-aacb-fe3953a7ffba",
   "metadata": {},
   "source": [
    "## KFold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2665bde-4469-4ada-943c-67cbecf0ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(\n",
    "    set(glob.glob(\"/app/_data/yolov5/runs/train/*/*/*.pt\")) - set(checked_paths)\n",
    ")\n",
    "# paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c0b02-143b-4385-85fe-4cdb797be5ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1567b2f5-443d-4dfe-a626-b4a6abf18a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "100% 8561/8561 [34:19<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/app/_data/yolov5/runs/train/yolov5l6_val2_norm_3000/weights/best.pt', 3000, 0.671, 0.41]]\n",
      "[['/app/_data/yolov5/runs/train/yolov5l6_val2_norm_3000/weights/best.pt', 3000, 0.671, 0.41]]\n"
     ]
    }
   ],
   "source": [
    "eval_IOU = 0.65\n",
    "res = []\n",
    "paths = [\"/app/_data/yolov5/runs/train/yolov5l6_val2_norm_3000/weights/best.pt\"]\n",
    "for a in range(len(paths)):\n",
    "    path = paths[a]\n",
    "    IMG_SIZE = 3000 if \"l6\" in path else 4480 if \"s6\" in path else 3840\n",
    "    model = torch.hub.load(\n",
    "        \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    "    )  # local repo\n",
    "    model.conf = 0.01\n",
    "    VIDEO_ID = int(path[path.index(\"val\") + 3])\n",
    "    if int(path[path.index(\"val\") + 3]) == 3:\n",
    "        VIDEO_ID = 0\n",
    "    df_test = df[df.video_id == VIDEO_ID]\n",
    "    df_sample = df_test\n",
    "    image_paths = df_sample.new_img_path.tolist()\n",
    "    gt = copy.deepcopy(df_sample.bboxes.tolist())\n",
    "    gtmem = copy.deepcopy(df_sample.bboxes.tolist())\n",
    "    TP = []  # Confidence scores of true positives\n",
    "    FP = []  # Confidence scores of true positives\n",
    "    FN = 0  # Count of false negative boxes\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        TEST_IMAGE_PATH = image_paths[i]\n",
    "        img = np.array(Image.open(TEST_IMAGE_PATH))\n",
    "        prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "        scores = prediction[:, 4]\n",
    "        bboxes = np.round(prediction[:, :4]).astype(\"int\")\n",
    "        bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "        bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "        gt0 = gt[i]\n",
    "        if len(bboxes) == 0:\n",
    "            # all gt are false negative\n",
    "            FN += len(gt0)\n",
    "        else:\n",
    "            bb = bboxes.copy().tolist()\n",
    "            for idx, b in enumerate(bb):\n",
    "                b.append(scores[idx])\n",
    "            bb.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "            if len(gt0) == 0:\n",
    "                # all bboxes are false positives\n",
    "                for b in bb:\n",
    "                    FP.append(b[4])\n",
    "            else:\n",
    "                # match bbox with gt\n",
    "                for b in bb:\n",
    "                    matched = False\n",
    "                    for g in gt0:\n",
    "                        # check whether gt box is already matched to an inference bb\n",
    "                        if len(g) == 4:\n",
    "                            # g bbox is unmatched\n",
    "                            if IOU_coco(b, g) >= eval_IOU:\n",
    "                                g.append(\n",
    "                                    b[4]\n",
    "                                )  # assign confidence values to g; marks g as matched\n",
    "                                matched = True\n",
    "                                TP.append(b[4])\n",
    "                                break\n",
    "                    if not matched:\n",
    "                        FP.append(b[4])\n",
    "                for g in gt0:\n",
    "                    if len(g) == 4:\n",
    "                        FN += 1\n",
    "    F2list = []\n",
    "    F2max = 0.0\n",
    "    F2maxat = -1.0\n",
    "\n",
    "    for c in np.arange(0.0, 1.0, 0.01):\n",
    "        FNcount = FN + sum(1 for i in TP if i < c)\n",
    "        TPcount = sum(1 for i in TP if i >= c)\n",
    "        FPcount = sum(1 for i in FP if i >= c)\n",
    "        R = TPcount / (TPcount + FNcount + 0.0001)\n",
    "        P = TPcount / (TPcount + FPcount + 0.0001)\n",
    "        F2 = (5 * P * R) / (4 * P + R + 0.0001)\n",
    "        F2list.append((c, F2))\n",
    "        if F2max < F2:\n",
    "            F2max = F2\n",
    "            F2maxat = c\n",
    "    res.append([path, IMG_SIZE, np.round(F2max, 3), np.round(F2maxat, 3)])\n",
    "    print(res)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f957a-42dc-460d-b20d-7b8ddb8a9c27",
   "metadata": {},
   "source": [
    "[['/app/_data/yolov5/runs/train/yolov5m6_val2/weights/last.pt', 4480, 0.691, 0.19],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m6_val2/weights/best.pt', 4480, 0.7, 0.27],  \n",
    "['/app/_data/yolov5/runs/train/yolov5l6_val110/weights/last.pt', 2880, 0.538, 0.09],  \n",
    "['/app/_data/yolov5/runs/train/yolov5l6_val110/weights/best.pt', 2880, 0.589, 0.12],  \n",
    "['/app/_data/yolov5/runs/train/yolov5s6_val29/weights/last.pt', 3840, 0.698, 0.18],  \n",
    "['/app/_data/yolov5/runs/train/yolov5s6_val29/weights/best.pt', 3840, 0.705, 0.35],  \n",
    "['/app/_data/yolov5/runs/train/yolov5l6_val2/weights/last.pt', 2880, 0.711, 0.36],  \n",
    "['/app/_data/yolov5/runs/train/yolov5l6_val2/weights/best.pt', 2880, 0.734, 0.34],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/last.pt', 3840, 0.487, 0.05],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/best.pt', 3840, 0.563, 0.08],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m6_val23/weights/last.pt', 4480, 0.674, 0.08],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m6_val23/weights/best.pt', 4480, 0.726, 0.23],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m6_val1/weights/last.pt', 4480, 0.526, 0.11],  \n",
    "['/app/_data/yolov5/runs/train/yolov5m6_val1/weights/best.pt', 4480, 0.582, 0.27]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1854de0f-4b5c-4a75-a06a-82a2128e678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m6_val2/weights/last.pt\", 4480, 0.691, 0.19],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m6_val2/weights/best.pt\", 4480, 0.7, 0.27],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5l6_val110/weights/last.pt\", 2880, 0.538, 0.09],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5l6_val110/weights/best.pt\", 2880, 0.589, 0.12],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5s6_val29/weights/last.pt\", 3840, 0.698, 0.18],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5s6_val29/weights/best.pt\", 3840, 0.705, 0.35],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5l6_val2/weights/last.pt\", 2880, 0.711, 0.36],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5l6_val2/weights/best.pt\", 2880, 0.734, 0.34],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/last.pt\", 3840, 0.487, 0.05],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/best.pt\", 3840, 0.563, 0.08],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m6_val23/weights/last.pt\", 4480, 0.674, 0.08],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m6_val23/weights/best.pt\", 4480, 0.726, 0.23],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m6_val1/weights/last.pt\", 4480, 0.526, 0.11],\n",
    "    [\"/app/_data/yolov5/runs/train/yolov5m6_val1/weights/best.pt\", 4480, 0.582, 0.27],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e471d0f-b034-42cf-953e-5c8766a71565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"/app/_data/yolov5/runs/train/yolov5l6_val2/weights/best.pt\"\n",
    "\"/app/_data/yolov5/runs/train/yolov5m6_val23/weights/best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa91d382-f598-41a5-af0a-12496fa880da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b7a5a-ed4c-4993-a02b-dc098e476abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_iou()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae0676-23ef-45be-aed6-a82403f9ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(preds, gts, iou_th):\n",
    "    num_tp = 0\n",
    "    num_fp = 0\n",
    "    num_fn = 0\n",
    "    for p, gt in zip(preds, gts):\n",
    "        if len(p) and len(gt):\n",
    "            iou_matrix = box_iou(p, gt)\n",
    "            tp = len(torch.where(iou_matrix.max(0)[0] >= iou_th)[0])\n",
    "            fp = len(p) - tp\n",
    "            fn = len(torch.where(iou_matrix.max(0)[0] < iou_th)[0])\n",
    "            num_tp += tp\n",
    "            num_fp += fp\n",
    "            num_fn += fn\n",
    "        elif len(p) == 0 and len(gt):\n",
    "            num_fn += len(gt)\n",
    "        elif len(p) and len(gt) == 0:\n",
    "            num_fp += len(p)\n",
    "    if (5 * num_tp + 4 * num_fn + num_fp) != 0:\n",
    "        score = 5 * num_tp / (5 * num_tp + 4 * num_fn + num_fp)\n",
    "    else:\n",
    "        score = np.nan\n",
    "    if (num_tp + num_fn) != 0:\n",
    "        recall = num_tp / (num_tp + num_fn)\n",
    "    else:\n",
    "        recall = np.nan\n",
    "    if (num_tp + num_fp) != 0:\n",
    "        precission = num_tp / (num_tp + num_fp)\n",
    "    else:\n",
    "        precission = np.nan\n",
    "\n",
    "    return score, precission, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758b7689-e106-475a-bbd9-d59608e90af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(preds, gts, iou_th):\n",
    "    num_tp = 0\n",
    "    num_fp = 0\n",
    "    num_fn = 0\n",
    "    for p, gt in zip(preds, gts):\n",
    "        if len(p) and len(gt):\n",
    "            iou_matrix = box_iou(p, gt)\n",
    "            tp = len(torch.where(iou_matrix.max(0)[0] >= iou_th)[0])\n",
    "            fp = len(p) - tp\n",
    "            fn = len(torch.where(iou_matrix.max(0)[0] < iou_th)[0])\n",
    "            num_tp += tp\n",
    "            num_fp += fp\n",
    "            num_fn += fn\n",
    "        elif len(p) == 0 and len(gt):\n",
    "            num_fn += len(gt)\n",
    "        elif len(p) and len(gt) == 0:\n",
    "            num_fp += len(p)\n",
    "    if (5 * num_tp + 4 * num_fn + num_fp) != 0:\n",
    "        score = 5 * num_tp / (5 * num_tp + 4 * num_fn + num_fp)\n",
    "    else:\n",
    "        score = np.nan\n",
    "    if (num_tp + num_fn) != 0:\n",
    "        recall = num_tp / (num_tp + num_fn)\n",
    "    else:\n",
    "        recall = np.nan\n",
    "    if (num_tp + num_fp) != 0:\n",
    "        precission = num_tp / (num_tp + num_fp)\n",
    "    else:\n",
    "        precission = np.nan\n",
    "\n",
    "    return score, precission, recall\n",
    "\n",
    "\n",
    "def evaluate_f2(confthre):\n",
    "    scores = []\n",
    "    prec05 = []\n",
    "    rec05 = []\n",
    "    prec03 = []\n",
    "    rec03 = []\n",
    "    iou_ths = np.arange(0.3, 0.85, 0.05)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dl_val:\n",
    "            model.eval()\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            preds = model(images)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                preds[i][\"boxes\"] = preds[i][\"boxes\"].int()\n",
    "                preds[i][\"boxes\"] = preds[i][\"boxes\"][preds[i][\"scores\"] > confthre]\n",
    "                score = [\n",
    "                    calculate_score(\n",
    "                        preds[i][\"boxes\"].unsqueeze(0),\n",
    "                        targets[i][\"boxes\"].unsqueeze(0),\n",
    "                        iou_th,\n",
    "                    )[0]\n",
    "                    for iou_th in iou_ths\n",
    "                ]\n",
    "                scores.append(np.nanmean(score))\n",
    "                prec05.append(\n",
    "                    calculate_score(\n",
    "                        preds[i][\"boxes\"].unsqueeze(0),\n",
    "                        targets[i][\"boxes\"].unsqueeze(0),\n",
    "                        0.5,\n",
    "                    )[1]\n",
    "                )\n",
    "                prec03.append(\n",
    "                    calculate_score(\n",
    "                        preds[i][\"boxes\"].unsqueeze(0),\n",
    "                        targets[i][\"boxes\"].unsqueeze(0),\n",
    "                        0.3,\n",
    "                    )[1]\n",
    "                )\n",
    "                rec05.append(\n",
    "                    calculate_score(\n",
    "                        preds[i][\"boxes\"].unsqueeze(0),\n",
    "                        targets[i][\"boxes\"].unsqueeze(0),\n",
    "                        0.5,\n",
    "                    )[2]\n",
    "                )\n",
    "                rec03.append(\n",
    "                    calculate_score(\n",
    "                        preds[i][\"boxes\"].unsqueeze(0),\n",
    "                        targets[i][\"boxes\"].unsqueeze(0),\n",
    "                        0.3,\n",
    "                    )[2]\n",
    "                )\n",
    "    print(\n",
    "        f\"F2 Score for confthre , {confthre}, :  {np.nanmean(scores):.3f} Precission .5: {np.nanmean(prec05):.3f} Precission .3: {np.nanmean(prec03):.3f}  Recall .5: {np.nanmean(rec05):.3f} Recall .3: {np.nanmean(rec03):.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "569a25ae-d4fa-45d6-b1f8-6927cb99c886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5_f2'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copytree(\"/app/_data/YOLOv5_kaggle/yolov5\", \"/app/_data/yolov5_f2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "577e0f1e-88ea-4931-ac0c-add4f628ec84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3018954-4f0f-4264-a480-113615fdddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"/app/_data/YOLOR_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5445a96-b0ec-4dcd-b864-30f6b0eb8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"/app/_data/yolov5/runs/train/yolov5l6_val1_3000/weights/best.pt\"]\n",
    "for a in range(len(paths)):\n",
    "    path = paths[a]\n",
    "    IMG_SIZE = 3008 * 2 if \"l6\" in path else 4480 * 2 if \"s6\" in path else 3840 * 2\n",
    "    model = torch.hub.load(\n",
    "        \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    "    )\n",
    "    model.conf = 0.01\n",
    "    VIDEO_ID = int(path[path.index(\"val\") + 3])\n",
    "    df_val = df[df.video_id == VIDEO_ID].reset_index(drop=True)\n",
    "    TP = []  # Confidence scores of true positives\n",
    "    FP = []  # Confidence scores of false positives\n",
    "    FN = 0  # Count of false negative boxes\n",
    "    for ix in tqdm(df_val.index().tolist()):\n",
    "        img_path = df_val.loc[ix, \"img_path\"]\n",
    "        img = np.array(Image.open(img_path))\n",
    "        prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "        scores = prediction[:, 4]\n",
    "        bboxes = np.round(prediction[:, :4]).astype(\"int\")\n",
    "        bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "        bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "        bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "        bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "        if df_val.loc[ix, \"len_annotation\"] == 0:\n",
    "            if bboxes.size != 0:\n",
    "                FP.append(scores)\n",
    "        else:\n",
    "            gts = np.array([list(x.values()) for x in df_val.loc[ix, \"annotations\"]])\n",
    "            gts[:, 2] = gts[:, 2] + gts[:, 0]\n",
    "            gts[:, 3] = gts[:, 3] + gts[:, 1]\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(box))\n",
    "            x = torch.where(iou_matrix >= iou_thr)\n",
    "            tp = np.unique(x[0]).shape[0]\n",
    "            fp = box.shape[0] - tp\n",
    "            fn = gt.shape[0] - tp\n",
    "            sc = np.stack([x[0].cpu().numpy(), scores], 1)\n",
    "            sc = sc[sc[:, 1].argsort()[::-1]]\n",
    "            unique_boxes = np.unique(sc[:, 0], return_index=True)[0].astype(\"int16\")\n",
    "            TP.append(list(sc[unique_boxes, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c747498-142f-411b-9e1d-74c70a69a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(\n",
    "    [\n",
    "        [307, 581, 334, 611],\n",
    "        [61, 219, 120, 269],\n",
    "        [175, 354, 223, 402],\n",
    "        [173, 314, 214, 345],\n",
    "        [64, 217, 120, 269],\n",
    "        [520, 900, 540, 930],\n",
    "        [62, 219, 120, 269],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "4a5275e0-6ff2-48b1-90d1-8940b6271256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = np.zeros(ious.shape[0], 'int16'), np.zeros(ious.shape[0], 'int16'), np.zeros(ious.shape[0], 'int16')\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, 'int16')\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n]= tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, 'int16')\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "98bf1c06-6e87-4229-8233-f198b25d4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.zeros(np.arange(0.3, 0.81, 0.05).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "1f502d5c-721e-4332-baaf-00fccdce900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full(8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "b8b80f1b-7446-44f5-a3c3-470dd6cfa455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5., 5., 5., 5., 5., 5., 5., 5., 5., 5.])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[:]=5\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "e840c099-f6fb-46b3-803d-c3b2bc87a5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.array([list(x.values()) for x in df.loc[5476, \"annotations\"]])\n",
    "gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "prediction = np.array(\n",
    "    [\n",
    "        [320, 596,  27,  30, 0.88],\n",
    "        [197, 379,  51,  46, 0.84],\n",
    "        [156, 299,  39,  31, 0.77],\n",
    "        [194, 329,  39,  31, 0.72],\n",
    "        [99, 244,  58,  50, 0.55],\n",
    "        [91, 244,  58,  50, 0.22],\n",
    "        [781, 356,  27,  31, 0.44],\n",
    "        [172, 356, 39, 31, 0.33],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7d3b2c21-cc81-4445-9d71-25d5aaf620ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.array([list(x.values()) for x in df.loc[5476, \"annotations\"]])\n",
    "# gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "# gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "prediction = np.array(\n",
    "    [\n",
    "        [320, 596,  27,  30, 0.3],\n",
    "        [197, 379,  51,  46, 0.2],\n",
    "        [156, 299,  39,  31, 0.01],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "913ea95d-ac80-4814-aa8b-5560ee629c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp, fn = tp_fp_fn(gt, prediction, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "a3b46395-8f66-4b78-80bf-6aff92387829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9286, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.9615, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.8920],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "20bd1f19-4a2c-4d59-8e0c-3624e91dfbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8),\n",
       " array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], dtype=int8))"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "590e0669-4a1e-477a-8133-19cd8188a2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "if len(fn):\n",
    "    print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "91fffe0d-daa7-4c7e-b290-657e9366359c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([list(x.values()) for x in df.loc[5476, \"annotations\"]]).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "f2721f3e-4598-46d4-9799-cc92ba061327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = np.array([list(x.values()) for x in df.loc[5476, \"annotations\"]])\n",
    "len(gt)\n",
    "gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "box = np.array(\n",
    "    [\n",
    "        [307, 581, 334, 611],\n",
    "        [61, 219, 120, 269],\n",
    "        [175, 354, 223, 402],\n",
    "        [173, 314, 214, 345],\n",
    "        [64, 217, 120, 269],\n",
    "        [520, 900, 540, 930],\n",
    "        [62, 219, 120, 269],\n",
    "    ]\n",
    ")\n",
    "scores = np.array([0.84, 0.26, 0.65, 0.61, 0.55, 0.45, 0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "de7ae344-7e94-42ab-acee-f895ce9c2641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[307, 581,  27,  30],\n",
       "       [172, 356,  51,  46],\n",
       "       [175, 314,  39,  31],\n",
       "       [ 62, 219,  58,  50],\n",
       "       [778, 341,  27,  31]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([list(x.values()) for x in df.loc[5476, \"annotations\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "22df10eb-e8a3-47c5-9c59-939e541eaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(\n",
    "    [\n",
    "        [307, 581, 27, 30, 0.88],\n",
    "        [172, 356, 51, 46, 0.84],\n",
    "        [175, 314, 39, 31, 0.77],\n",
    "        [62, 219, 58, 50, 0.72],\n",
    "        [778, 341, 27, 31, 0.55],\n",
    "        [760, 320, 27, 31, 0.22],\n",
    "        [168, 333, 39, 31, 0.77],\n",
    "        [176, 316, 39, 31, 0.33],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "627eccea-ca2e-4364-af8f-437e09de87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(box))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1eb2da75-2aaa-4613-82b9-81976b4aec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_thr = 0.3\n",
    "x = torch.where(iou_matrix >= iou_thr)\n",
    "tp = np.unique(x[0]).shape[0]\n",
    "fp = box.shape[0] - tp\n",
    "fn = gt.shape[0] - tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6be7e661-4735-4cf2-871d-556071c94053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3, 1)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6918d204-5e01-4572-9989-958a313485af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.9042, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.9512, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.9831, 0.0000, 0.0000, 0.9296, 0.0000, 1.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2c29c706-8e78-4e3e-8289-a2d6f1664deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3, 3, 3]), tensor([0, 2, 3, 1, 4, 6]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "840f31f1-ac01-445b-a29f-d0437bdf581c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 3, 3],\n",
       "       [0, 2, 3, 1, 4, 6]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.stack([x[0].cpu().numpy(), x[1].cpu().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f469d1eb-46e3-4230-b81a-d9d3eceda2a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_165/3334554675.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "np.stack([x[0].cpu().numpy(), x[1].cpu().numpy(), scores], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ebff1a95-4b5a-47c7-92d8-79c09fcfd26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.84, 0.65, 0.61, 0.55])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = np.stack([x[0].cpu().numpy(), x[1].cpu().numpy(), scores], 1)\n",
    "sc = sc[sc[:, 2].argsort()[::-1]]\n",
    "unique_boxes = np.unique(sc[:, 0], return_index=True)[0].astype(\"int16\")\n",
    "sc[unique_boxes, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "2524fd56-6002-4155-b27b-643d21adc73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.84],\n",
       "       [2.  , 3.  , 0.65],\n",
       "       [3.  , 1.  , 0.61],\n",
       "       [3.  , 4.  , 0.55],\n",
       "       [3.  , 6.  , 0.45],\n",
       "       [1.  , 2.  , 0.26]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "37236285-974a-4a06-8844-9c330ce25e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3], dtype=int16)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3f567828-c80f-4910-a558-c80b63f15e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 ])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.3, 0.81, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb849140-88b4-4425-9b16-c8ad3ddb6c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70389df0-e687-4865-8365-0130fb60080d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e5a1c38c-9e86-4005-b0d8-fda0b8a77ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.       , 6.       , 1.       ],\n",
       "       [0.       , 0.       , 1.       ],\n",
       "       [3.       , 1.       , 0.9830508],\n",
       "       [2.       , 3.       , 0.9512195],\n",
       "       [3.       , 4.       , 0.9296149],\n",
       "       [1.       , 2.       , 0.9041769]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.       , 0.       , 1.       ],\n",
       "       [1.       , 2.       , 0.9041769],\n",
       "       [2.       , 3.       , 0.9512195],\n",
       "       [3.       , 6.       , 1.       ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.where(iou_matrix >= 0.3)\n",
    "if x[0].shape[0]:\n",
    "    matches = (\n",
    "        torch.cat((torch.stack(x, 1), iou_matrix[x[0], x[1]][:, None]), 1).cpu().numpy()\n",
    "    )\n",
    "    matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "    matches\n",
    "    matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "    matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "    matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "    matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dd4694ee-ca26-41c2-932c-98f0c9195c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3],\n",
       "       [0, 2, 3, 1],\n",
       "       [1, 0, 0, 0]], dtype=int16)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.transpose().astype(np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "be1aaf3e-c5ed-4d8f-ad04-db43a69976f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_classes = [0]\n",
    "n = matches.shape[0] > 0\n",
    "m0, m1, _ = matches.transpose().astype(np.int16)\n",
    "for i, gc in enumerate(gt_classes):\n",
    "    j = m0 == i\n",
    "#     if n and sum(j) == 1:\n",
    "#         self.matrix[detection_classes[m1[j]], gc] += 1  # correct\n",
    "#     else:\n",
    "#         self.matrix[self.nc, gc] += 1  # background FP\n",
    "\n",
    "# if n:\n",
    "#     for i, dc in enumerate(detection_classes):\n",
    "#         if not any(m1 == i):\n",
    "#             self.matrix[dc, self.nc] += 1  # background FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4b28b69-0213-459c-9333-3e760bfd366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(j) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708a97f-b2bf-4176-a549-85ba1b4a24ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d43721d9-a52e-48b9-93ef-6302000db0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[307., 581., 334., 611.],\n",
       "        [172., 356., 223., 402.],\n",
       "        [175., 314., 214., 345.],\n",
       "        [ 62., 219., 120., 269.],\n",
       "        [778., 341., 805., 372.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[307., 581., 334., 611.],\n",
       "        [ 61., 219., 120., 269.],\n",
       "        [175., 354., 223., 402.],\n",
       "        [173., 314., 214., 345.],\n",
       "        [ 64., 217., 120., 269.],\n",
       "        [520., 900., 540., 930.],\n",
       "        [ 62., 219., 120., 269.]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(gt)\n",
    "torch.Tensor(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a720d284-ac9b-4485-be76-af5eff8efeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[3.07e+02, 5.81e+02, 3.34e+02, 6.11e+02, 8.40e-01, 0.00e+00],\n",
       "       [6.10e+01, 2.19e+02, 1.20e+02, 2.69e+02, 2.60e-01, 0.00e+00],\n",
       "       [1.75e+02, 3.54e+02, 2.23e+02, 4.02e+02, 6.50e-01, 0.00e+00],\n",
       "       [1.73e+02, 3.14e+02, 2.14e+02, 3.45e+02, 6.10e-01, 0.00e+00],\n",
       "       [6.40e+01, 2.17e+02, 1.20e+02, 2.69e+02, 5.50e-01, 0.00e+00],\n",
       "       [5.20e+02, 9.00e+02, 5.40e+02, 9.30e+02, 4.50e-01, 0.00e+00],\n",
       "       [6.20e+01, 2.19e+02, 1.20e+02, 2.69e+02, 8.00e-01, 0.00e+00]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt = np.array([list(x.values()) for x in df.loc[5476, \"annotations\"]])\n",
    "len(gt)\n",
    "gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "box = np.array(\n",
    "    [\n",
    "        [307, 581, 334, 611],\n",
    "        [61, 219, 120, 269],\n",
    "        [175, 354, 223, 402],\n",
    "        [173, 314, 214, 345],\n",
    "        [64, 217, 120, 269],\n",
    "        [520, 900, 540, 930],\n",
    "        [62, 219, 120, 269],\n",
    "    ]\n",
    ")\n",
    "scores = np.array([0.84, 0.26, 0.65, 0.61, 0.55, 0.45, 0.8])\n",
    "preds = np.zeros([box.shape[0], box.shape[1] + 2])\n",
    "gt0 = np.zeros([gt.shape[0], gt.shape[1] + 1])\n",
    "gt0[:, 0] = 0\n",
    "gt0[:, 1:] = gt\n",
    "preds[:, :4] = box\n",
    "preds[:, 4] = scores\n",
    "preds[:, 5] = 0\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "89b993af-5b37-4689-93cd-4fa9a666d104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 307., 581., 334., 611.],\n",
       "       [  0., 172., 356., 223., 402.],\n",
       "       [  0., 175., 314., 214., 345.],\n",
       "       [  0.,  62., 219., 120., 269.],\n",
       "       [  0., 778., 341., 805., 372.]])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa53456-c1e3-4f47-93ea-ab8ef28fa0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070345ce-e98a-4fb7-8f7b-796380ff9a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f628b9-1b82-4df6-b403-53becc18b7e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328042b-d20f-48e9-b536-ad203d5f0388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10adda0-9cbb-472d-bfbe-0d8a8d6cf468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7965fe47-757a-4373-940d-775d9b73897f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfbb89f-9089-4426-b199-4ef62f062070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c0323-a437-4bc4-8a39-5a71fec08c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b81ab01-c8f7-4de9-a952-25cb6fb8af31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658a49e1-3d41-4bff-ba0b-30aecae7ead0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559acb1c-8344-4bc9-9a72-de156d9d84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.where(iou > 0.3)\n",
    "    if x[0].shape[0]:\n",
    "        matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n",
    "        if x[0].shape[0] > 1:\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
    "            matches = matches[matches[:, 2].argsort()[::-1]]\n",
    "            matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
    "    else:\n",
    "        matches = np.zeros((0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2fbd1a3-a51d-4b80-9158-8965f53c8145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[307, 581, 334, 611],\n",
       "       [172, 356, 223, 402],\n",
       "       [175, 314, 214, 345],\n",
       "       [ 62, 219, 120, 269],\n",
       "       [778, 341, 805, 372]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            tp = len(torch.where(iou_matrix.max(0)[0] >= iou_th)[0])\n",
    "            fp = len(p) - tp\n",
    "            fn = len(torch.where(iou_matrix.max(0)[0] < iou_th)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837c0dd0-e00c-4205-b9ac-0d775c636bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(iou_matrix.max(0)[0] >= 0.3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52aae48e-0a19-4589-b0ec-744d1a13b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_iou_array(boxes, new_box):\n",
    "    # bb interesection over union\n",
    "    xA = np.maximum(boxes[:, 0], new_box[0])\n",
    "    yA = np.maximum(boxes[:, 1], new_box[1])\n",
    "    xB = np.minimum(boxes[:, 2], new_box[2])\n",
    "    yB = np.minimum(boxes[:, 3], new_box[3])\n",
    "\n",
    "    interArea = np.maximum(xB - xA, 0) * np.maximum(yB - yA, 0)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "    boxBArea = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])\n",
    "\n",
    "    iou = interArea / (boxAArea + boxBArea - interArea)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6f344e3-bb74-4016-97cb-c33f9bc92b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([          0,     0.85016,           0,           0,           0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb_iou_array(gt, box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03785657-f00d-4ec4-ba85-df563f65a93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.8501577287066246)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_matching_box_quickly(gt, box, 0.3, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c67ddbf-b2b6-4e2d-80ba-59b565cc3a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24266MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      "100% 8232/8232 [1:54:18<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['/app/_data/yolov5/runs/train/yolov5l6_val1_3000/weights/best.pt', 6016, 0.546, 0.42]]\n",
      "[['/app/_data/yolov5/runs/train/yolov5l6_val1_3000/weights/best.pt', 6016, 0.546, 0.42]]\n"
     ]
    }
   ],
   "source": [
    "eval_IOU = 0.65\n",
    "res = []\n",
    "paths = [\"/app/_data/yolov5/runs/train/yolov5l6_val1_3000/weights/best.pt\"]\n",
    "for a in range(len(paths)):\n",
    "    path = paths[a]\n",
    "    IMG_SIZE = 3008 * 2 if \"l6\" in path else 4480 * 2 if \"s6\" in path else 3840 * 2\n",
    "    model = torch.hub.load(\n",
    "        \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    "    )\n",
    "    model.conf = 0.01\n",
    "    VIDEO_ID = int(path[path.index(\"val\") + 3])\n",
    "\n",
    "    df_test = df[df.video_id == VIDEO_ID]\n",
    "    df_sample = df_test\n",
    "    image_paths = df_sample.img_path.tolist()\n",
    "    gt = copy.deepcopy(df_sample.bboxes.tolist())\n",
    "    gtmem = copy.deepcopy(df_sample.bboxes.tolist())\n",
    "    TP = []  # Confidence scores of true positives\n",
    "    FP = []  # Confidence scores of true positives\n",
    "    FN = 0  # Count of false negative boxes\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        TEST_IMAGE_PATH = image_paths[i]\n",
    "        img = np.array(Image.open(TEST_IMAGE_PATH))\n",
    "        prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "        prediction[prediction[:, 4] > 0.01]\n",
    "        scores = prediction[:, 4]\n",
    "        bboxes = np.round(prediction[:, :4]).astype(\"int\")\n",
    "        bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "        bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "        gt0 = gt[i]\n",
    "        if len(bboxes) == 0:\n",
    "            # all gt are false negative\n",
    "            FN += len(gt0)\n",
    "        else:\n",
    "            bb = bboxes.copy().tolist()\n",
    "            for idx, b in enumerate(bb):\n",
    "                b.append(scores[idx])\n",
    "            bb.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "            if len(gt0) == 0:\n",
    "                # all bboxes are false positives\n",
    "                for b in bb:\n",
    "                    FP.append(b[4])\n",
    "            else:\n",
    "                # match bbox with gt\n",
    "                for b in bb:\n",
    "                    matched = False\n",
    "                    for g in gt0:\n",
    "                        # check whether gt box is already matched to an inference bb\n",
    "                        if len(g) == 4:\n",
    "                            # g bbox is unmatched\n",
    "                            if IOU_coco(b, g) >= eval_IOU:\n",
    "                                g.append(\n",
    "                                    b[4]\n",
    "                                )  # assign confidence values to g; marks g as matched\n",
    "                                matched = True\n",
    "                                TP.append(b[4])\n",
    "                                break\n",
    "                    if not matched:\n",
    "                        FP.append(b[4])\n",
    "                for g in gt0:\n",
    "                    if len(g) == 4:\n",
    "                        FN += 1\n",
    "    F2list = []\n",
    "    F2max = 0.0\n",
    "    F2maxat = -1.0\n",
    "\n",
    "    for c in np.arange(0.0, 1.0, 0.01):\n",
    "        FNcount = FN + sum(1 for i in TP if i < c)\n",
    "        TPcount = sum(1 for i in TP if i >= c)\n",
    "        FPcount = sum(1 for i in FP if i >= c)\n",
    "        R = TPcount / (TPcount + FNcount + 0.0001)\n",
    "        P = TPcount / (TPcount + FPcount + 0.0001)\n",
    "        F2 = (5 * P * R) / (4 * P + R + 0.0001)\n",
    "        F2list.append((c, F2))\n",
    "        if F2max < F2:\n",
    "            F2max = F2\n",
    "            F2maxat = c\n",
    "    res.append([path, IMG_SIZE, np.round(F2max, 3), np.round(F2maxat, 3)])\n",
    "    print(res)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ddecee2-aa10-4c7d-9a2e-1ec26fccd8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24266MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients\n",
      "Adding AutoShape... \n",
      " 21% 1758/8561 [34:33<2:13:44,  1.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_556/1098101966.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mTEST_IMAGE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_IMAGE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxywh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mnew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mclass\u001b[0m \u001b[0mArrayData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "paths = glob.glob(\"/app/_data/yolov5/runs/train/*/*/*.pt\")\n",
    "paths = [\"/app/_data/yolov5/runs/train/yolov5l6_val2_30002/weights/best.pt\"]\n",
    "\n",
    "eval_IOU = 0.65\n",
    "res = []\n",
    "for a in range(len(paths)):\n",
    "    path = paths[a]\n",
    "    IMG_SIZE = 7200 if \"l6\" in path else 4480 if \"s6\" in path else 3840 * 3\n",
    "    model = torch.hub.load(\n",
    "        \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    "    )  # local repo\n",
    "    model.conf = 0.01\n",
    "    VIDEO_ID = int(path[path.index(\"val\") + 3])\n",
    "\n",
    "    df_test = df[df.video_id == VIDEO_ID]\n",
    "    df_sample = df_test\n",
    "    image_paths = df_sample.img_path.tolist()\n",
    "    gt = copy.deepcopy(df_sample.bboxes.tolist())\n",
    "    gtmem = copy.deepcopy(df_sample.bboxes.tolist())\n",
    "    TP = []  # Confidence scores of true positives\n",
    "    FP = []  # Confidence scores of true positives\n",
    "    FN = 0  # Count of false negative boxes\n",
    "    for i in tqdm(range(len(image_paths))):\n",
    "        TEST_IMAGE_PATH = image_paths[i]\n",
    "        img = np.array(Image.open(TEST_IMAGE_PATH))\n",
    "        prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "        scores = prediction[:, 4]\n",
    "        bboxes = np.round(prediction[:, :4]).astype(\"int\")\n",
    "        bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "        bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "        gt0 = gt[i]\n",
    "        if len(bboxes) == 0:\n",
    "            # all gt are false negative\n",
    "            FN += len(gt0)\n",
    "        else:\n",
    "            bb = bboxes.copy().tolist()\n",
    "            for idx, b in enumerate(bb):\n",
    "                b.append(scores[idx])\n",
    "            bb.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "            if len(gt0) == 0:\n",
    "                # all bboxes are false positives\n",
    "                for b in bb:\n",
    "                    FP.append(b[4])\n",
    "            else:\n",
    "                # match bbox with gt\n",
    "                for b in bb:\n",
    "                    matched = False\n",
    "                    for g in gt0:\n",
    "                        # check whether gt box is already matched to an inference bb\n",
    "                        if len(g) == 4:\n",
    "                            # g bbox is unmatched\n",
    "                            if IOU_coco(b, g) >= eval_IOU:\n",
    "                                g.append(\n",
    "                                    b[4]\n",
    "                                )  # assign confidence values to g; marks g as matched\n",
    "                                matched = True\n",
    "                                TP.append(b[4])\n",
    "                                break\n",
    "                    if not matched:\n",
    "                        FP.append(b[4])\n",
    "                for g in gt0:\n",
    "                    if len(g) == 4:\n",
    "                        FN += 1\n",
    "    F2list = []\n",
    "    F2max = 0.0\n",
    "    F2maxat = -1.0\n",
    "\n",
    "    for c in np.arange(0.0, 1.0, 0.01):\n",
    "        FNcount = FN + sum(1 for i in TP if i < c)\n",
    "        TPcount = sum(1 for i in TP if i >= c)\n",
    "        FPcount = sum(1 for i in FP if i >= c)\n",
    "        R = TPcount / (TPcount + FNcount + 0.0001)\n",
    "        P = TPcount / (TPcount + FPcount + 0.0001)\n",
    "        F2 = (5 * P * R) / (4 * P + R + 0.0001)\n",
    "        F2list.append((c, F2))\n",
    "        if F2max < F2:\n",
    "            F2max = F2\n",
    "            F2maxat = c\n",
    "    res.append([path, IMG_SIZE, np.round(F2max, 3), np.round(F2maxat, 3)])\n",
    "    print(res)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c7713ee-59db-4eb9-a702-bbb17b58e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 8232/8232 [34:40<00:00,  3.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Confidence scores of true positives, false positives and count false negatives\n",
    "eval_IOU = 0.65\n",
    "IMG_SIZE = 3840\n",
    "TP = []  # Confidence scores of true positives\n",
    "FP = []  # Confidence scores of true positives\n",
    "FN = 0  # Count of false negative boxes\n",
    "for i in tqdm(range(len(image_paths))):\n",
    "    TEST_IMAGE_PATH = image_paths[i]\n",
    "    img = np.array(Image.open(TEST_IMAGE_PATH))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    scores = prediction[:, 4]\n",
    "    bboxes = np.round(prediction[:, :4]).astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "\n",
    "    gt0 = gt[i]\n",
    "    if len(bboxes) == 0:\n",
    "        # all gt are false negative\n",
    "        FN += len(gt0)\n",
    "    else:\n",
    "        bb = bboxes.copy().tolist()\n",
    "        for idx, b in enumerate(bb):\n",
    "            b.append(scores[idx])\n",
    "        bb.sort(key=lambda x: x[4], reverse=True)\n",
    "\n",
    "        if len(gt0) == 0:\n",
    "            # all bboxes are false positives\n",
    "            for b in bb:\n",
    "                FP.append(b[4])\n",
    "        else:\n",
    "            # match bbox with gt\n",
    "            for b in bb:\n",
    "                matched = False\n",
    "                for g in gt0:\n",
    "                    # check whether gt box is already matched to an inference bb\n",
    "                    if len(g) == 4:\n",
    "                        # g bbox is unmatched\n",
    "                        if IOU_coco(b, g) >= eval_IOU:\n",
    "                            g.append(\n",
    "                                b[4]\n",
    "                            )  # assign confidence values to g; marks g as matched\n",
    "                            matched = True\n",
    "                            TP.append(b[4])\n",
    "                            break\n",
    "                if not matched:\n",
    "                    FP.append(b[4])\n",
    "            for g in gt0:\n",
    "                if len(g) == 4:\n",
    "                    FN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32aa0df8-3fa0-4e2f-be0b-b8e23f0fc783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 277,
       "width": 385
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "F2list = []\n",
    "F2max = 0.0\n",
    "F2maxat = -1.0\n",
    "\n",
    "for c in np.arange(0.0, 1.0, 0.01):\n",
    "    FNcount = FN + sum(1 for i in TP if i < c)\n",
    "    TPcount = sum(1 for i in TP if i >= c)\n",
    "    FPcount = sum(1 for i in FP if i >= c)\n",
    "    R = TPcount / (TPcount + FNcount + 0.0001)\n",
    "    P = TPcount / (TPcount + FPcount + 0.0001)\n",
    "    F2 = (5 * P * R) / (4 * P + R + 0.0001)\n",
    "    F2list.append((c, F2))\n",
    "    if F2max < F2:\n",
    "        F2max = F2\n",
    "        F2maxat = c\n",
    "\n",
    "plt.scatter(*zip(*F2list))\n",
    "plt.title(\"CONF vs F2 score\")\n",
    "plt.xlabel(\"CONF\")\n",
    "plt.ylabel(\"F2\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac897db9-d68b-4fb9-a616-b92dc16d3f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 max is 0.571 at CONF = 0.3 for model /app/_data/YOLOv5Weights/best5m63840_val1.pt and image size 3840\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"F2 max is {np.round(F2max,3)} at CONF = {np.round(F2maxat,3)} for model {path} and image size {IMG_SIZE}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc394ed8-4a61-49e4-882a-9a32f9aac182",
   "metadata": {},
   "source": [
    "F2 max is 0.571 at CONF = 0.3 for model /app/_data/YOLOv5Weights/best5m63840_val1.pt and image size 3840\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
