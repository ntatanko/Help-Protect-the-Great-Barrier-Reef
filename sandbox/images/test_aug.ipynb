{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e70d3950-b3ee-4b3b-bd63-3a60d58f0183",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from torchvision.ops import box_iou\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd507f2e-336b-4c65-939a-c6bd8b941458",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_img(img, bbox, pred=None):\n",
    "    img_h, img_w = img.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "    ax.imshow(img)\n",
    "    for i in range(len(bbox)):\n",
    "        x_c, y_c, w, h = bbox[i]\n",
    "        rect = plt.Rectangle(\n",
    "            [(x_c - w / 2) * img_w, (y_c - h / 2) * img_h],\n",
    "            w * img_w,\n",
    "            h * img_h,\n",
    "            ec=\"b\",\n",
    "            fc=\"none\",\n",
    "            lw=2.0,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    if pred is not None:\n",
    "        for i in range(len(pred)):\n",
    "            x_c, y_c, w, h = pred[i]\n",
    "            rect = plt.Rectangle(\n",
    "                [x_c - w / 2, y_c - h / 2],\n",
    "                w,\n",
    "                h,\n",
    "                ec=\"r\",\n",
    "                fc=\"none\",\n",
    "                lw=2.0,\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b800857-3dff-4170-8782-2ddc958bae6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DF_PART = \"/app/_data/tensorflow-great-barrier-reef/train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78ee602c-cf99-4639-8805-304c06276e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DF_PART)\n",
    "df[\"img_path\"] = (\n",
    "    \"/app/_data/tensorflow-great-barrier-reef/train_images/video_\"\n",
    "    + df.video_id.astype(\"str\")\n",
    "    + \"/\"\n",
    "    + df.video_frame.astype(\"str\")\n",
    "    + \".jpg\"\n",
    ")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"len_annotation\"] = df[\"annotations\"].str.len()\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"-\", \"_\", regex=True)\n",
    "df[\"new_img_path\"] = f\"/app/_data/images/\" + df[\"image_id\"] + \".jpg\"\n",
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66af4840-1bc6-45eb-90ca-c47dcb58085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = (\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "    )\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, \"int16\")\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n] = tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, \"int16\")\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3705cb38-75bd-4dbd-9413-ab6d76e98d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"r\") as f:\n",
    "    res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659cb02d-785a-4b3b-b8b8-cb2a0f1bacd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d3fafc8-270c-49eb-9cf0-1a640e2c9f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.2, scale_limit=[0.1, 0.2], rotate_limit=45, p=1.0,\n",
    "        ),\n",
    "        A.HueSaturationValue(\n",
    "            hue_shift_limit=3, sat_shift_limit=3, val_shift_limit=5, p=1\n",
    "        ),\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.01, contrast_limit=0.05, brightness_by_max=True, p=1\n",
    "        ),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"yolo\", min_visibility=0.5, label_fields=[\"class_labels\"]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9eaf95-6f1d-4eba-84f2-d18991473e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "path = f\"/app/_data/yolov5_f2/runs/train/{NAME}/weights/best.pt\"\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481260fc-cb66-455a-93e3-b6fbdfe8ff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w = 1280\n",
    "img_h = 720\n",
    "for ix in tqdm(df_test.index):\n",
    "    image_path = df_test.loc[ix, \"img_path\"]\n",
    "    img_name = df_test.loc[ix, \"image_id\"]\n",
    "    annotations = df_test.loc[ix, \"annotations\"]\n",
    "    img = np.array(Image.open(image_path))\n",
    "    bboxes = np.zeros([len(annotations), 5])\n",
    "    if len(annotations):\n",
    "        for i in range(len(annotations)):\n",
    "            xmin = annotations[i][\"x\"] / img_w\n",
    "            ymin = annotations[i][\"y\"] / img_h\n",
    "            width = annotations[i][\"width\"] / img_w\n",
    "            height = annotations[i][\"height\"] / img_h\n",
    "            width = width if (width + xmin) <= 1 else (1 - xmin)\n",
    "            height = height if (height + ymin) <= 1 else (1 - ymin)\n",
    "            x_center = xmin + width / 2\n",
    "            y_center = ymin + height / 2\n",
    "            bboxes[i:, 0] = 0\n",
    "            bboxes[i:, 1] = x_center\n",
    "            bboxes[i:, 2] = y_center\n",
    "            bboxes[i:, 3] = width\n",
    "            bboxes[i:, 4] = height\n",
    "        for n in range(5):\n",
    "            transformed = transform(\n",
    "                image=img,\n",
    "                bboxes=bboxes[:, 1:],\n",
    "                class_labels=bboxes[:, 0],\n",
    "            )\n",
    "            a_img, a_bbox = transformed[\"image\"], transformed[\"bboxes\"]\n",
    "            prediction = model(a_img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "            prediction = prediction[prediction[:, 4] > 0.1]\n",
    "            gt = np.array(a_bbox)\n",
    "            if gt.size:\n",
    "                gt[:, 0] *= 1280\n",
    "                gt[:, 1] *= 720\n",
    "                gt[:, 2] *= 1280\n",
    "                gt[:, 3] *= 720\n",
    "                gt[:, 0] = gt[:, 0] - gt[:, 2] / 2\n",
    "                gt[:, 1] = gt[:, 1] - gt[:, 3] / 2\n",
    "                gt[:, 2] = gt[:, 0] + gt[:, 2]\n",
    "                gt[:, 3] = gt[:, 1] + gt[:, 3]\n",
    "\n",
    "            for n, c_th in enumerate(conf_thres):\n",
    "                TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "                res[n, 0, :] += TP\n",
    "                res[n, 1, :] += FP\n",
    "                res[n, 2, :] += FN\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7c1c2-3647-49ff-bf9b-75f476fa1b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2da6704-ac68-4ebb-bdd7-596f1a39e73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64cd732-9a48-46b2-92bf-f4b998175663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe1d73-e582-4ba2-a00e-4ecfcae33de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1967c-bc95-4e1f-8e3b-45c7b3c49ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24265MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "Adding AutoShape... \n",
      " 37% 3207/8561 [10:20<17:17,  5.16it/s]"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "path = f\"/app/_data/yolov5_f2/runs/train/{NAME}/weights/best.pt\"\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "# chose validation set\n",
    "df_test = val.copy()\n",
    "# computing f2 score\n",
    "for ix in tqdm(df_test.index.tolist()):\n",
    "    img = np.array(Image.open(df_test.loc[ix, \"img_path\"]))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    prediction = prediction[prediction[:, 4] > 0.1]\n",
    "    gt = np.array([list(x.values()) for x in df_test.loc[ix, \"annotations\"]])\n",
    "    if gt.size:\n",
    "        gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "        gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "    for n, c_th in enumerate(conf_thres):\n",
    "        TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "        res[n, 0, :] += TP\n",
    "        res[n, 1, :] += FP\n",
    "        res[n, 2, :] += FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276c865-af77-4aa8-a404-382a5a99b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)\n",
    "if path not in res_dict:\n",
    "    res_dict[path] = {\n",
    "        IMG_SIZE: {\n",
    "            \"best\": [\n",
    "                np.round(conf_thres[np.argmax(F2)], 2),\n",
    "                np.round(np.max(F2), 4),\n",
    "            ],\n",
    "            \"all\": list(np.round(F2, 4)),\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    res_dict[path][IMG_SIZE] = {\n",
    "        \"best\": [\n",
    "            np.round(conf_thres[np.argmax(F2)], 2),\n",
    "            np.round(np.max(F2), 4),\n",
    "        ],\n",
    "        \"all\": list(np.round(F2, 4)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efdff8b-c082-448a-b5f2-c702d60e35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict[path][IMG_SIZE]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
