{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39fb378-4509-4a17-a45d-0192f02fd218",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import (\n",
    "    GroupKFold,\n",
    "    KFold,\n",
    "    StratifiedGroupKFold,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "from random import sample\n",
    "\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b1c37a-3aa0-410a-9f46-3cde4939f80a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DF_PART = \"/app/_data/tensorflow-great-barrier-reef/train.csv\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "LABEL_FOLDER = \"labels\"\n",
    "SEED = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b3349a-9b18-43a4-bb94-79ad3005f9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_DF_PART)\n",
    "df[\"img_path\"] = (\n",
    "    \"/app/_data/tensorflow-great-barrier-reef/train_images/video_\"\n",
    "    + df.video_id.astype(\"str\")\n",
    "    + \"/\"\n",
    "    + df.video_frame.astype(\"str\")\n",
    "    + \".jpg\"\n",
    ")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"len_annotation\"] = df[\"annotations\"].str.len()\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"-\", \"_\", regex=True)\n",
    "df[\"new_img_path\"] = f\"/app/_data/{IMAGE_FOLDER}/\" + df[\"image_id\"] + \".jpg\"\n",
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "df[\"no_label\"] = df[\"len_annotation\"].apply(lambda x: True if x == 0 else False)\n",
    "R = df[df[\"len_annotation\"] == 0].shape[0] / df[df[\"len_annotation\"] != 0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f646bfa-90a1-4d9f-aba3-7546ac5433d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label_change\"] = df[\"label\"] & df[\"no_label\"].shift(1) & df[\"no_label\"].shift(\n",
    "    2\n",
    ") | df[\"no_label\"] & df[\"label\"].shift(1) & df[\"label\"].shift(2)\n",
    "df[\"sequense_change\"] = df[\"sequence\"] != df[\"sequence\"].shift(1)\n",
    "df[\"start_subseq\"] = df[\"sequense_change\"] | df[\"label_change\"]\n",
    "df.loc[df.index[-1], \"start_subseq\"] = True\n",
    "df[\"start_subseq\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e5816e-a400-4dab-8450-ab073017475c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = 0\n",
    "for subsequence_id, end_idx in enumerate(df[df[\"start_subseq\"]].index):\n",
    "    df.loc[start_idx:end_idx, \"subsequence_id\"] = subsequence_id\n",
    "    start_idx = end_idx\n",
    "\n",
    "df[\"subsequence_id\"] = df[\"subsequence_id\"].astype(int)\n",
    "df[\"subsequence_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cfcc5f-7072-4f1e-a62e-072eb39a4c5a",
   "metadata": {},
   "source": [
    "# Random split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540fa6ce-cf30-49fe-982a-28f3c03e59c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% 6642/20000 [01:51<03:44, 59.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    9.482819\n",
       "label             8.228893\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.217636022514071"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[1, 9, 19, 23, 26, 42, 45, 73, 88, 91, 102, 105, 106, 123, 125, 134, 135, 137]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "ll = 0\n",
    "for i in tqdm(range(20000)):\n",
    "    n_val = np.random.randint(15,25,1)[0]\n",
    "    val_seq = sample(df.subsequence_id.unique().tolist(), n_val)\n",
    "    train_seq = list(set(df.subsequence_id.tolist()) - set(val_seq))\n",
    "    l = (\n",
    "        df.query(\"subsequence_id in @train_seq\")[\"label\"].sum()\n",
    "        / df.query(\"subsequence_id in @val_seq\")[\"label\"].sum()\n",
    "    )\n",
    "    ll = (\n",
    "        df.query(\"subsequence_id in @train_seq\")[\"len_annotation\"].sum()\n",
    "        / df.query(\"subsequence_id in @val_seq\")[\"len_annotation\"].sum()\n",
    "    )\n",
    "    r = df.query(\"subsequence_id in @val_seq and len_annotation == 0\").shape[0] /df.query(\"subsequence_id in @val_seq and len_annotation != 0\").shape[0] \n",
    "    if 8<=l <=10 and 8<=ll <=10 and 3.5 <= r <= 4.3:\n",
    "        break\n",
    "\n",
    "df.query(\"subsequence_id in @train_seq\")[[\"len_annotation\", \"label\"]].sum() / df.query(\n",
    "    \"subsequence_id in @val_seq\"\n",
    ")[[\"len_annotation\", \"label\"]].sum()\n",
    "\n",
    "df.query(\"subsequence_id in @val_seq and len_annotation == 0\").shape[0] /df.query(\"subsequence_id in @val_seq and len_annotation != 0\").shape[0]\n",
    "\n",
    "sorted(val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a065a24-31a2-4403-86f0-f3b216896db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    10763\n",
       "label              4386\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    1135\n",
       "label              533\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    9.482819\n",
       "label             8.228893\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KFOLD = 'rand'\n",
    "train_idx = df.query(\"subsequence_id in @train_seq and len_annotation>0\").index.tolist()\n",
    "val_idx = df.query(\"subsequence_id in @val_seq\").index.tolist()\n",
    "train = df.loc[train_idx]\n",
    "val = df.loc[val_idx]\n",
    "train[[\"len_annotation\", \"label\"]].sum()\n",
    "val[[\"len_annotation\", \"label\"]].sum()\n",
    "train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d554ae3-48b9-456f-ba72-bdb58ca1b810",
   "metadata": {},
   "source": [
    "# GroupKFold on subsequence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d5cc1ad-f93d-4f59-847e-af8cc8453570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = df.query(\"len_annotation>0\").reset_index(drop=True)\n",
    "# kf = GroupKFold(n_splits=10)\n",
    "# list_train_ids = []\n",
    "# list_val_ids = []\n",
    "# for kfold, (train_idx, val_idx) in enumerate(\n",
    "#     kf.split(train_df, y=train_df.label, groups=train_df.subsequence_id)\n",
    "# ):\n",
    "#     list_train_ids.append(train_idx)\n",
    "#     list_val_ids.append(val_idx)\n",
    "#     print(\n",
    "#         kfold,\n",
    "#         \"\\n\",\n",
    "#         train_df.loc[train_idx, [\"len_annotation\", \"label\"]].sum()\n",
    "#         / train_df.loc[val_idx, [\"len_annotation\", \"label\"]].sum(),\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd811fb-a079-4d7f-b7a0-614137a3a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD = 1\n",
    "# train_idx = list_train_ids[KFOLD]\n",
    "# val_idx = list_val_ids[KFOLD]\n",
    "# train = train_df.loc[train_idx]\n",
    "# val = train_df.loc[val_idx]\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()\n",
    "# sorted(val.subsequence_id.unique())\n",
    "# sorted(train.subsequence_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a9cba-8a67-4983-8182-7bb94de3101d",
   "metadata": {},
   "source": [
    "# Split on video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390046dc-75f6-41dc-92fa-8849e525850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_ID = 2\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\"),\n",
    "#         df.query(\"video_id!=@VIDEO_ID and len_annotation==0\").sample(\n",
    "#             int(df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\").shape[0] * 0.07)\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac=1)\n",
    "# val = df.query(\"video_id==@VIDEO_ID\").sample(frac=1)\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdb1ae-ac4a-4795-8b67-3b306f92f80c",
   "metadata": {},
   "source": [
    "# StratifiedKFold on subsequence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc984b6-3a95-454b-97d1-652cc50ca4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = (\n",
    "#     df.groupby(\"subsequence_id\")\n",
    "#     .agg({\"label\": \"max\", \"len_annotation\": \"sum\", \"video_frame\": \"count\"})\n",
    "#     .astype(int)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# n_splits = 10\n",
    "# y=df_split[\"label\"]\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# for fold_id, (train_idx, val_idx) in enumerate(\n",
    "#     skf.split(df_split[\"subsequence_id\"], y=y)\n",
    "# ):\n",
    "#     subseq_val_idx = df_split[\"subsequence_id\"].iloc[val_idx]\n",
    "#     df.loc[df[\"subsequence_id\"].isin(subseq_val_idx), \"fold\"] = fold_id\n",
    "\n",
    "# df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "# for fold in range(10):\n",
    "#     print(f\"\\nFold {fold}\")\n",
    "#     df.query(\"fold != @fold\")[[\"len_annotation\", \"label\"]].sum() / df.query(\n",
    "#         \"fold == @fold\"\n",
    "#     )[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a27f626-6fa1-444f-939d-191d802b7ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD = 3\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"fold != @KFOLD and len_annotation!=0\"),\n",
    "#         df.query(\"fold != @KFOLD and len_annotation==0\").sample(\n",
    "#             int(df.query(\"fold != @KFOLD and len_annotation!=0\").shape[0] * 0.07)\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac=1)\n",
    "# val = df.query(\"fold == @KFOLD\").sample(frac=1)\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb7a1d9-5fa5-42c0-b227-499a7c98e137",
   "metadata": {},
   "source": [
    "## save image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5f9521-ef45-4c3d-a14d-ea0b94c60f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5/data/reef_data_rand.yaml'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/app/_data/val_rand.txt'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt = f\"/app/_data/train_{KFOLD}.txt\"\n",
    "val_txt = train_txt.replace(\"train_\", \"val_\")\n",
    "data_yaml_path = f\"/app/_data/yolov5/data/reef_data_{KFOLD}.yaml\"\n",
    "data_yaml_path\n",
    "val_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d6de231-778e-41d2-ac62-905b8300228a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = train[\"new_img_path\"].tolist()\n",
    "val_img_path = val[\"new_img_path\"].tolist()\n",
    "np.savetxt(\n",
    "    train_txt,\n",
    "    train_img_path,\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "np.savetxt(val_txt, val_img_path, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4e997-5e2f-4d54-b530-9adec9a0766a",
   "metadata": {},
   "source": [
    "## Custimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d902af-8f06-42ed-8e06-e162d4eb4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab02a815-d5fd-491f-b35c-f4138e4a95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate {data_yaml_path}\n",
    "\n",
    "train: {train_txt}  # training directory\n",
    "val: {val_txt}  # validation directory\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: [\"starfish\"]  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f42f32-555a-4f6b-a5af-eceac5481056",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate /app/_data/yolov5/data/hyps/hyp.custom.yaml\n",
    "# YOLOv5  by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "lr0: 0.01\n",
    "lrf: 0.1\n",
    "momentum: 0.937\n",
    "weight_decay: 0.0005\n",
    "warmup_epochs: 3.0\n",
    "warmup_momentum: 0.8\n",
    "warmup_bias_lr: 0.1\n",
    "box: 0.05\n",
    "cls: 0.5\n",
    "cls_pw: 1.0\n",
    "obj: 1.0\n",
    "obj_pw: 1.0\n",
    "iou_t: 0.2\n",
    "anchor_t: 4.0\n",
    "fl_gamma: 0.0\n",
    "hsv_h: 0.015\n",
    "hsv_s: 0.7\n",
    "hsv_v: 0.4\n",
    "degrees: 0.0\n",
    "translate: 0.1\n",
    "scale: 0.5\n",
    "shear: 0.0\n",
    "perspective: 0.0\n",
    "flipud: 0.5\n",
    "fliplr: 0.5\n",
    "mosaic: 1.0\n",
    "mixup: 0.5\n",
    "copy_paste: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beab6da9-ebfe-4747-a919-fbcc3ad6bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb\n",
    "clear_output()\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b003516b-47c8-452f-8513-af1d0c934931",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app/_data/yolov5/\n",
    "!pip install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c870746-0869-4061-a5fb-a9e26e439e0e",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfcf35d3-d207-4bcf-984c-4a7e797f1a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5x6_2400_seq_id_rand_0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS = \"yolov5x6.pt\"\n",
    "IMG_SIZE = 2400\n",
    "NAME = f\"{WEIGHTS[:-3]}_{IMG_SIZE}_seq_id_{KFOLD}_0\"\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e4256a6-a62a-47d4-a460-9dfb0efbec67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x6.pt, cfg=, data=/app/_data/yolov5/data/reef_data_seq_id_rand.yaml, hyp=data/hyps/hyp.finetune.yaml, epochs=40, batch_size=1, imgsz=2400, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=True, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=yolov5x6_2400_seq_id_rand_0, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=10, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5  v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0032, lrf=0.12, momentum=0.843, weight_decay=0.00036, warmup_epochs=2.0, warmup_momentum=0.5, warmup_bias_lr=0.05, box=0.0296, cls=0.243, cls_pw=0.631, obj=0.301, obj_pw=0.911, iou_t=0.2, anchor_t=2.91, fl_gamma=0.0, hsv_h=0.0138, hsv_s=0.664, hsv_v=0.464, degrees=0.373, translate=0.245, scale=0.898, shear=0.602, perspective=0.0, flipud=0.00856, fliplr=0.5, mosaic=1.0, mixup=0.243, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myolov5x6_2400_seq_id_rand_0\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 猸锔 View project at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5/runs/1anrfp77\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /app/_data/yolov5/wandb/run-20220208_163848-1anrfp77\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
      "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
      "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
      "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
      "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
      "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
      "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
      "  7                -1  1   5531520  models.common.Conv                      [640, 960, 3, 2]              \n",
      "  8                -1  4  11070720  models.common.C3                        [960, 960, 4]                 \n",
      "  9                -1  1  11061760  models.common.Conv                      [960, 1280, 3, 2]             \n",
      " 10                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
      " 11                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
      " 12                -1  1   1230720  models.common.Conv                      [1280, 960, 1, 1]             \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  4  11992320  models.common.C3                        [1920, 960, 4, False]         \n",
      " 16                -1  1    615680  models.common.Conv                      [960, 640, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
      " 20                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
      " 24                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
      " 27                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  4  11377920  models.common.C3                        [1280, 960, 4, False]         \n",
      " 30                -1  1   8296320  models.common.Conv                      [960, 960, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  4  20495360  models.common.C3                        [1920, 1280, 4, False]        \n",
      " 33  [23, 26, 29, 32]  1     57672  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [320, 640, 960, 1280]]\n",
      "Model Summary: 733 layers, 140035432 parameters, 140035432 gradients, 208.3 GFLOPs\n",
      "\n",
      "Transferred 955/963 items from yolov5x6.pt\n",
      "Scaled weight_decay = 0.00036\n",
      "WARNING: --img-size 2400 must be multiple of max stride 64, updating to 2432\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 159 weight (no decay), 163 weight, 163 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8)), RandomBrightnessContrast(always_apply=False, p=0.01, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True), RandomGamma(always_apply=False, p=0.01, gamma_limit=(80, 120), eps=None)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/app/_data/train_37_seq_id_rand_l6yaml' images and labels...437\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /app/_data/train_37_seq_id_rand_l6yaml.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/app/_data/val_37_seq_id_rand_l6yaml' images and labels...543 fou\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /app/_data/val_37_seq_id_rand_l6yaml.cache\n",
      "Plotting labels to runs/train/yolov5x6_2400_seq_id_rand_0/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.81 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset \n",
      "Image sizes 2432 train, 2432 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov5x6_2400_seq_id_rand_0\u001b[0m\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/39     22.8G   0.02803   0.03137         0         2      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472          0          0          0          0          0\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/39     22.9G    0.0212   0.01471         0         5      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108     0.0114      0.074    0.00268   0.000857\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/39     22.9G   0.01638   0.01151         0         2      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.682      0.427      0.458      0.215\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/39     22.9G   0.01407  0.009054         0         5      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108       0.79      0.536      0.588      0.298\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/39     22.9G   0.01303  0.008096         0         4      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108       0.85      0.542      0.608      0.296\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/39     22.9G   0.01225  0.007595         0         2      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.868      0.565      0.648      0.329\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/39     22.9G   0.01176  0.007362         0         1      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.901      0.584       0.67      0.333\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/39     22.9G    0.0113  0.007017         0         8      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.873      0.593      0.666      0.295\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/39     22.9G   0.01108  0.006859         0         2      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.824      0.592      0.655      0.302\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/39     22.9G   0.01091   0.00677         0        22      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.847      0.604      0.678      0.339\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/39     22.9G   0.01076  0.006749         0         9      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.786      0.631      0.669      0.321\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/39     22.9G   0.01035  0.006457         0         4      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.876      0.636      0.697      0.342\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/39     22.9G   0.01053  0.006514         0         6      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.833      0.644      0.689      0.347\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/39     22.9G   0.01028  0.006343         0         0      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.879      0.614       0.69       0.35\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/39     22.9G      0.01  0.006063         0         2      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.829      0.649      0.707      0.353\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/39     22.9G  0.009985  0.006336         0         1      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.867      0.614      0.689      0.337\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/39     22.9G  0.009869  0.006195         0         4      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.812       0.62      0.677      0.322\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/39     22.9G  0.009621  0.006005         0         4      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.813      0.647      0.693      0.335\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/39     22.9G  0.009471  0.005841         0         4      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.813      0.634      0.691      0.345\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/39     22.9G  0.009519  0.006055         0         1      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108       0.85      0.624      0.691      0.339\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/39     22.9G  0.009405  0.005959         0         2      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.832      0.641      0.696       0.34\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/39     22.9G   0.00918  0.005639         0         0      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.822      0.641      0.694      0.338\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/39     22.9G  0.009252  0.005818         0         5      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.833      0.649      0.702      0.338\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/39     22.9G  0.009238  0.005793         0        12      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.854       0.63        0.7      0.344\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/39     22.9G  0.009249  0.005773         0         5      2432: 100%|\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.811       0.65      0.698      0.339\n",
      "Stopping training early as no improvement observed in last 10 epochs. Best results observed at epoch 14, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n",
      "\n",
      "25 epochs completed in 24.815 hours.\n",
      "Optimizer stripped from runs/train/yolov5x6_2400_seq_id_rand_0/weights/last.pt, 282.5MB\n",
      "Optimizer stripped from runs/train/yolov5x6_2400_seq_id_rand_0/weights/best.pt, 282.5MB\n",
      "\n",
      "Validating runs/train/yolov5x6_2400_seq_id_rand_0/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 574 layers, 139970872 parameters, 0 gradients, 208.1 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2472       1108      0.837      0.644      0.707      0.353\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1285... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             best/epoch 14\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best/mAP_0.5 0.70701\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      best/mAP_0.5:0.95 0.35344\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         best/precision 0.82915\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            best/recall 0.64892\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.70692\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.35339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.83689\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.6444\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.00925\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.00577\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.00297\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.00142\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00146\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 209 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33myolov5x6_2400_seq_id_rand_0\u001b[0m: \u001b[34mhttps://wandb.ai/tatanko/YOLOv5/runs/1anrfp77\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220208_163848-1anrfp77/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "Results saved to \u001b[1mruns/train/yolov5x6_2400_seq_id_rand_0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img {IMG_SIZE} \\\n",
    "                --batch 1\\\n",
    "                --epochs 40 \\\n",
    "                --data {data_yaml_path} \\\n",
    "                --weights {WEIGHTS} \\\n",
    "                --name {NAME} \\\n",
    "                --hyp data/hyps/hyp.finetune.yaml \\\n",
    "                --single-cls \\\n",
    "                --patience 10 \\\n",
    "                --workers 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4268c15-62c8-457b-ae15-9b8e8f1fd9eb",
   "metadata": {},
   "source": [
    "# check f2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49fa10cb-1662-4023-adb7-84aa419592e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = (\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "    )\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, \"int16\")\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n] = tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, \"int16\")\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdb3c081-8fd9-4d37-a1a4-099b0e67fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"r\") as f:\n",
    "    res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8182fc61-2130-414c-bfbe-5aa37a6fef1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 574 layers, 139970872 parameters, 0 gradients, 208.1 GFLOPs\n",
      "Adding AutoShape... \n",
      "100% 2472/2472 [39:52<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "path = f\"/app/_data/yolov5/runs/train/{NAME}/weights/best.pt\"\n",
    "IMG_SIZE = IMG_SIZE*2\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "# chose validation set\n",
    "df_test = val.copy()\n",
    "# computing f2 score\n",
    "for ix in tqdm(df_test.index.tolist()):\n",
    "    img = np.array(Image.open(df_test.loc[ix, \"img_path\"]))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    prediction = prediction[prediction[:, 4] > 0.1]\n",
    "    gt = np.array([list(x.values()) for x in df_test.loc[ix, \"annotations\"]])\n",
    "    if gt.size:\n",
    "        gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "        gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "    for n, c_th in enumerate(conf_thres):\n",
    "        TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "        res[n, 0, :] += TP\n",
    "        res[n, 1, :] += FP\n",
    "        res[n, 2, :] += FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63e390a4-f082-480a-8039-0076fc26d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)\n",
    "if path not in res_dict:\n",
    "    res_dict[path] = {\n",
    "        IMG_SIZE: {\n",
    "            \"best\": [\n",
    "                np.round(conf_thres[np.argmax(F2)], 2),\n",
    "                np.round(np.max(F2), 4),\n",
    "            ],\n",
    "            \"all\": list(np.round(F2, 4)),\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    res_dict[path][IMG_SIZE] = {\n",
    "        \"best\": [\n",
    "            np.round(conf_thres[np.argmax(F2)], 2),\n",
    "            np.round(np.max(F2), 4),\n",
    "        ],\n",
    "        \"all\": list(np.round(F2, 4)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91143a15-e9a0-4527-8212-e467ab44816c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': [0.3, 0.6223],\n",
       " 'all': [0.583,\n",
       "  0.5893,\n",
       "  0.596,\n",
       "  0.6001,\n",
       "  0.604,\n",
       "  0.6085,\n",
       "  0.6127,\n",
       "  0.6143,\n",
       "  0.6177,\n",
       "  0.6198,\n",
       "  0.6212,\n",
       "  0.6184,\n",
       "  0.6193,\n",
       "  0.6193,\n",
       "  0.6194,\n",
       "  0.6201,\n",
       "  0.6204,\n",
       "  0.6208,\n",
       "  0.6203,\n",
       "  0.6206,\n",
       "  0.6223,\n",
       "  0.622,\n",
       "  0.6209,\n",
       "  0.6194,\n",
       "  0.6189,\n",
       "  0.6176,\n",
       "  0.6151,\n",
       "  0.6141,\n",
       "  0.6128,\n",
       "  0.6104,\n",
       "  0.6086,\n",
       "  0.6037,\n",
       "  0.6016,\n",
       "  0.6013,\n",
       "  0.5936,\n",
       "  0.5908,\n",
       "  0.5879,\n",
       "  0.583,\n",
       "  0.5794,\n",
       "  0.5763,\n",
       "  0.5708,\n",
       "  0.5656,\n",
       "  0.5609,\n",
       "  0.5575,\n",
       "  0.5508,\n",
       "  0.541,\n",
       "  0.5354,\n",
       "  0.5233,\n",
       "  0.5104,\n",
       "  0.4978,\n",
       "  0.4838]}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict[path][4800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cc51f0be-344d-47bc-ae6d-1944b04dc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f58c3f79-b062-4767-8521-e0f3e221bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /app/_data/yolov5/runs/train/{NAME}/weights/best.pt /app/_data/YOLOv5full_train_weights/{NAME}.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3deeae9-fce5-4b19-a636-d2adf87ba155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
