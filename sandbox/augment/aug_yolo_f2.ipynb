{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f37ba-fb4c-4925-9f32-d8917a597f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33298966-6d6a-4e86-945a-93895bb6d21c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5d1c63-13cf-4fea-869b-29e7fabb36e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"images\"\n",
    "LABEL_FOLDER = \"labels\"\n",
    "SEED = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204bf9d4-30aa-4b3a-83d9-a15cff496403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/app/_data/train_alb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92f9287a-ac33-476f-b079-f21b3bb2e62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>img_path</th>\n",
       "      <th>len_annotation</th>\n",
       "      <th>new_img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>width_max</th>\n",
       "      <th>width_min</th>\n",
       "      <th>height_max</th>\n",
       "      <th>height_min</th>\n",
       "      <th>width_mean</th>\n",
       "      <th>height_mean</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_0.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_2.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0_3</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_3.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0_4</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_4.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38230</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10631</td>\n",
       "      <td>2859</td>\n",
       "      <td>2_10631_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10631_1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[51, 643, 44, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38231</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10631</td>\n",
       "      <td>2859</td>\n",
       "      <td>2_10631_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10631_2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[51, 643, 44, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38232</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2_10632_0</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10632_0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[38, 681, 45, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38233</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2_10632_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10632_1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[38, 681, 45, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38234</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2_10632_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10632_2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[38, 681, 45, 37]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38235 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id  sequence  video_frame  sequence_frame   image_id annotations  \\\n",
       "0             0     40258            0               0        0_0          []   \n",
       "1             0     40258            1               1        0_1          []   \n",
       "2             0     40258            2               2        0_2          []   \n",
       "3             0     40258            3               3        0_3          []   \n",
       "4             0     40258            4               4        0_4          []   \n",
       "...         ...       ...          ...             ...        ...         ...   \n",
       "38230         2     29859        10631            2859  2_10631_1          []   \n",
       "38231         2     29859        10631            2859  2_10631_2          []   \n",
       "38232         2     29859        10632            2860  2_10632_0          []   \n",
       "38233         2     29859        10632            2860  2_10632_1          []   \n",
       "38234         2     29859        10632            2860  2_10632_2          []   \n",
       "\n",
       "                                                img_path  len_annotation  \\\n",
       "0      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "1      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "2      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "3      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "4      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "...                                                  ...             ...   \n",
       "38230  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38231  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38232  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38233  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38234  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "\n",
       "                                    new_img_path  label  width_max  width_min  \\\n",
       "0                      /app/_data/images/0_0.jpg      0        NaN        NaN   \n",
       "1                      /app/_data/images/0_1.jpg      0        NaN        NaN   \n",
       "2                      /app/_data/images/0_2.jpg      0        NaN        NaN   \n",
       "3                      /app/_data/images/0_3.jpg      0        NaN        NaN   \n",
       "4                      /app/_data/images/0_4.jpg      0        NaN        NaN   \n",
       "...                                          ...    ...        ...        ...   \n",
       "38230  /app/_data/augmented/images/2_10631_1.jpg      1       44.0       44.0   \n",
       "38231  /app/_data/augmented/images/2_10631_2.jpg      1       44.0       44.0   \n",
       "38232  /app/_data/augmented/images/2_10632_0.jpg      1       45.0       45.0   \n",
       "38233  /app/_data/augmented/images/2_10632_1.jpg      1       45.0       45.0   \n",
       "38234  /app/_data/augmented/images/2_10632_2.jpg      1       45.0       45.0   \n",
       "\n",
       "       height_max  height_min  width_mean  height_mean                 bbox  \n",
       "0             NaN         NaN         NaN          NaN                  NaN  \n",
       "1             NaN         NaN         NaN          NaN                  NaN  \n",
       "2             NaN         NaN         NaN          NaN                  NaN  \n",
       "3             NaN         NaN         NaN          NaN                  NaN  \n",
       "4             NaN         NaN         NaN          NaN                  NaN  \n",
       "...           ...         ...         ...          ...                  ...  \n",
       "38230        37.0        37.0        44.0         37.0  [[51, 643, 44, 37]]  \n",
       "38231        37.0        37.0        44.0         37.0  [[51, 643, 44, 37]]  \n",
       "38232        37.0        37.0        45.0         37.0  [[38, 681, 45, 37]]  \n",
       "38233        37.0        37.0        45.0         37.0  [[38, 681, 45, 37]]  \n",
       "38234        37.0        37.0        45.0         37.0  [[38, 681, 45, 37]]  \n",
       "\n",
       "[38235 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf54c948-6499-4591-a6af-d3478c3f5933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "df[\"no_label\"] = df[\"len_annotation\"].apply(lambda x: True if x == 0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f867e630-f3a5-4a26-a3e8-e882b6e3c091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label_change\"] = df[\"label\"] & df[\"no_label\"].shift(1) & df[\"no_label\"].shift(\n",
    "    2\n",
    ") | df[\"no_label\"] & df[\"label\"].shift(1) & df[\"label\"].shift(2)\n",
    "df[\"sequense_change\"] = df[\"sequence\"] != df[\"sequence\"].shift(1)\n",
    "df[\"start_subseq\"] = df[\"sequense_change\"] | df[\"label_change\"]\n",
    "df.loc[df.index[-1], \"start_subseq\"] = True\n",
    "df[\"start_subseq\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc455d0-5630-4b11-b55b-9e7df2630536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = 0\n",
    "for subsequence_id, end_idx in enumerate(df[df[\"start_subseq\"]].index):\n",
    "    df.loc[start_idx:end_idx, \"subsequence_id\"] = subsequence_id\n",
    "    start_idx = end_idx\n",
    "\n",
    "df[\"subsequence_id\"] = df[\"subsequence_id\"].astype(int)\n",
    "df[\"subsequence_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b35c1-9cf3-4608-a677-9573040a2cc8",
   "metadata": {},
   "source": [
    "## KFold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9860dfd-0345-4978-b370-fe5773803f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = GroupKFold(n_splits=4)\n",
    "# list_train_ids = []\n",
    "# list_val_ids= []\n",
    "# for train_idx, val_idx in (\n",
    "#     kf.split(train_df, y=train_df.len_annotation, groups=train_df.sequence)\n",
    "# ):\n",
    "#     list_train_ids.append(train_idx)\n",
    "#     list_val_ids.append(val_idx)\n",
    "#     print(train_df.loc[val_idx, \"len_annotation\"].values.sum(), train_df.loc[train_idx, \"len_annotation\"].values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9da6b685-5b4b-4d35-9794-3e55f4766d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    37735\n",
       "label             16949\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    9781\n",
       "label             2704\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    3.857990\n",
       "label             6.268121\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_ID = 2\n",
    "train = pd.concat(\n",
    "    [\n",
    "        df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\"),\n",
    "        df.query(\"video_id!=@VIDEO_ID and len_annotation==0\").sample(\n",
    "            int(df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\").shape[0] * 0.03)\n",
    "        ),\n",
    "    ]\n",
    ").sample(frac=1)\n",
    "val = df.query(\"video_id==@VIDEO_ID\").sample(frac=1)\n",
    "train[[\"len_annotation\", \"label\"]].sum()\n",
    "val[[\"len_annotation\", \"label\"]].sum()\n",
    "train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707848cc-343e-4e38-b363-97f67e0c1854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = (\n",
    "#     df.groupby(\"subsequence_id\")\n",
    "#     .agg({\"label\": \"max\", \"len_annotation\": \"sum\", \"video_frame\": \"count\"})\n",
    "#     .astype(int)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# n_splits = 10\n",
    "# y=df_split[\"label\"]\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# for fold_id, (train_idx, val_idx) in enumerate(\n",
    "#     skf.split(df_split[\"subsequence_id\"], y=y)\n",
    "# ):\n",
    "#     subseq_val_idx = df_split[\"subsequence_id\"].iloc[val_idx]\n",
    "#     df.loc[df[\"subsequence_id\"].isin(subseq_val_idx), \"fold\"] = fold_id\n",
    "\n",
    "# df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "# for fold in range(10):\n",
    "#     print(f\"\\nFold {fold}\")\n",
    "#     df.query(\"fold != @fold\")[[\"len_annotation\", \"label\"]].sum() / df.query(\n",
    "#         \"fold == @fold\"\n",
    "#     )[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecd9c93b-2d4b-446f-aae8-741d078edb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD = 3\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"fold != @KFOLD and len_annotation!=0\"),\n",
    "#         df.query(\"fold != @KFOLD and len_annotation==0\").sample(\n",
    "#             int(df.query(\"fold != @KFOLD and len_annotation!=0\").shape[0] * 0.07)\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac=1)\n",
    "# val = df.query(\"fold == @KFOLD\").sample(frac=1)\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b611a00-b72f-4537-957b-315c3888670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train.index.tolist()\n",
    "val_ids = val.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5068e09d-d318-4f25-a9e8-b5e93ffcd9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5_f2/data/reef_data_aug_val2.yaml'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/app/_data/val_aug_37_val2.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt =  f\"/app/_data/train_aug_{SEED}_val{VIDEO_ID}.txt\"\n",
    "val_txt = train_txt.replace('train_', 'val_')\n",
    "data_yaml_path = f'/app/_data/yolov5_f2/data/reef_data_aug_val{VIDEO_ID}.yaml'\n",
    "data_yaml_path\n",
    "val_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8cdb44f-b737-4fe3-b237-9b291251bac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = df.loc[train_ids, \"new_img_path\"].tolist()\n",
    "val_img_path = df.loc[val_ids, \"new_img_path\"].tolist()\n",
    "np.savetxt(\n",
    "    train_txt,\n",
    "    train_img_path,\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "np.savetxt(val_txt, val_img_path, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0363a264-96bb-430a-83c0-ff7c1253b0ec",
   "metadata": {},
   "source": [
    "## Custimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41d92312-0d8e-4894-bde6-ea9164b96345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79d4feb6-e862-42ac-951a-5df0ea91f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate {data_yaml_path}\n",
    "\n",
    "train: {train_txt} # training directory\n",
    "val: {val_txt} # validation directory\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: ['starfish']  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13600dc9-c819-493f-9765-e0d36b045397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: /app/_data/train_aug_37_val2.txt # training directory\n",
      "val: /app/_data/val_aug_37_val2.txt # validation directory\n",
      "\n",
      "# Classes\n",
      "nc: 1  # number of classes\n",
      "names: ['starfish']  # class names\n"
     ]
    }
   ],
   "source": [
    "!cat {data_yaml_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6f64311-9acb-4a81-8434-3542baecac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
      "# Hyperparameters for COCO training from scratch\n",
      "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
      "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
      "\n",
      "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
      "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
      "momentum: 0.937  # SGD momentum/Adam beta1\n",
      "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
      "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
      "warmup_momentum: 0.8  # warmup initial momentum\n",
      "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
      "box: 0.05  # box loss gain\n",
      "cls: 0.5  # cls loss gain\n",
      "cls_pw: 1.0  # cls BCELoss positive_weight\n",
      "obj: 1.0  # obj loss gain (scale with pixels)\n",
      "obj_pw: 1.0  # obj BCELoss positive_weight\n",
      "iou_t: 0.20  # IoU training threshold\n",
      "anchor_t: 4.0  # anchor-multiple threshold\n",
      "# anchors: 3  # anchors per output layer (0 to ignore)\n",
      "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
      "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
      "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
      "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
      "degrees: 0.0  # image rotation (+/- deg)\n",
      "translate: 0.1  # image translation (+/- fraction)\n",
      "scale: 0.5  # image scale (+/- gain)\n",
      "shear: 0.0  # image shear (+/- deg)\n",
      "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
      "flipud: 0.0  # image flip up-down (probability)\n",
      "fliplr: 0.5  # image flip left-right (probability)\n",
      "mosaic: 1.0  # image mosaic (probability)\n",
      "mixup: 0.0  # image mixup (probability)\n",
      "copy_paste: 0.0  # segment copy-paste (probability)\n"
     ]
    }
   ],
   "source": [
    "!cat /app/_data/yolov5_f2/data/hyps/hyp.scratch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8054c9f8-729e-4b7c-9918-7017dbec0030",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate /app/_data/yolov5_f2/data/hyps/hyp.aug_custom.yaml\n",
    "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "lr0: 0.005  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.3  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.3  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.1  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.1  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa23287a-4aad-4c28-9254-2609b31ded7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb\n",
    "clear_output()\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61ccd8f8-135e-4e5c-9740-34f87333f8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app/_data/yolov5_f2/\n",
    "!pip install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9111f08-5f06-4059-840c-23c692b7f73b",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b461a44a-5ee1-49f0-8d72-470e9504b0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5l6_2560_val2_rect_aug_f2'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS = \"yolov5l6.pt\"\n",
    "IMG_SIZE = 2560\n",
    "NAME = f\"{WEIGHTS[:-3]}_{IMG_SIZE}_val{VIDEO_ID}_rect_aug_f2\"\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3304e041-fb99-4ffb-b4ea-c3992167c409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in glob.glob('/app/_data/yolov5_f2/runs/train/*rect_aug_f2*/'):\n",
    "#     shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45a0e96-d9a7-42a4-a44e-929db3216904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/app/_data/yolov5_f2/runs/train/NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43baaccb-1ee7-4737-99b0-316bd1c97d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l6.pt, cfg=, data=/app/_data/yolov5_f2/data/reef_data_aug_val2.yaml, hyp=data/hyps/hyp.aug_custom.yaml, epochs=80, batch_size=4, imgsz=2560, rect=True, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=True, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=yolov5l6_2560_val2_rect_aug_f2, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=10, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5 üöÄ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.005, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.3, hsv_v=0.3, degrees=0.0, translate=0.1, scale=0.1, shear=0.0, perspective=0.0, flipud=0.1, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myolov5l6_2560_val2_rect_aug_f2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5/runs/48og3qhg\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /app/_data/yolov5_f2/wandb/run-20220203_191910-48og3qhg\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   3540480  models.common.Conv                      [512, 768, 3, 2]              \n",
      "  8                -1  3   5611008  models.common.C3                        [768, 768, 3]                 \n",
      "  9                -1  1   7079936  models.common.Conv                      [768, 1024, 3, 2]             \n",
      " 10                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      " 11                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 12                -1  1    787968  models.common.Conv                      [1024, 768, 1, 1]             \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  3   6200832  models.common.C3                        [1536, 768, 3, False]         \n",
      " 16                -1  1    394240  models.common.Conv                      [768, 512, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 24                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 27                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  3   5807616  models.common.C3                        [1024, 768, 3, False]         \n",
      " 30                -1  1   5309952  models.common.Conv                      [768, 768, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  3  10496000  models.common.C3                        [1536, 1024, 3, False]        \n",
      " 33  [23, 26, 29, 32]  1     46152  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [256, 512, 768, 1024]]\n",
      "Model Summary: 607 layers, 76162504 parameters, 76162504 gradients, 110.2 GFLOPs\n",
      "\n",
      "Transferred 787/795 items from yolov5l6.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight (no decay), 135 weight, 135 bias\n",
      "WARNING: --rect is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 3)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 3)), ToGray(always_apply=False, p=0.01), RandomBrightnessContrast(always_apply=False, p=0.1, brightness_limit=(-0.05, 0.05), contrast_limit=(-0.05, 0.05), brightness_by_max=True), RandomGamma(always_apply=False, p=0.05, gamma_limit=(80, 120), eps=None), ImageCompression(always_apply=False, p=0.05, quality_lower=95, quality_upper=100, compression_type=0), Sharpen(always_apply=False, p=0.05, alpha=(0.2, 0.3), lightness=(0.5, 1.0)), HueSaturationValue(always_apply=False, p=0.01, hue_shift_limit=(-5, 5), sat_shift_limit=(-5, 5), val_shift_limit=(-5, 5)), RGBShift(always_apply=False, p=0.01, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), Flip(always_apply=False, p=0.01), ShiftScaleRotate(always_apply=False, p=0.5, shift_limit_x=(-0.2, 0.2), shift_limit_y=(-0.2, 0.2), scale_limit=(0.10000000000000009, 0.19999999999999996), rotate_limit=(10, 45), interpolation=1, border_mode=4, value=None, mask_value=None), RandomRain(always_apply=False, p=0.01, slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.6, rain_type='drizzle')\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/app/_data/train_aug_37_val2' images and labels...16949 found, \u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /app/_data/train_aug_37_val2.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/app/_data/val_aug_37_val2.cache' images and labels... 2704 found\u001b[0m\n",
      "Plotting labels to runs/train/yolov5l6_2560_val2_rect_aug_f2/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.48 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Image sizes 2560 train, 2560 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov5l6_2560_val2_rect_aug_f2\u001b[0m\n",
      "Starting training for 80 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/79     21.3G   0.05166    0.0477         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.565      0.583      0.598      0.324\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/79     20.5G   0.03688   0.02994         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.619      0.664      0.688      0.512\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/79     20.5G   0.03466   0.02771         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.647      0.587      0.607      0.428\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/79     20.5G   0.03327   0.02732         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.662      0.668      0.692      0.533\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/79     20.5G   0.02968   0.02346         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.644      0.714      0.729       0.57\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/79     20.5G   0.02712   0.02122         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.737      0.722       0.75      0.569\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/79     20.5G   0.02527   0.01967         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.746      0.704      0.747      0.577\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/79     20.5G   0.02408   0.01865         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.702      0.701      0.727      0.571\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/79     20.5G   0.02349   0.01793         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.751      0.691      0.736       0.57\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/79     20.5G   0.02259   0.01721         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.742      0.671      0.708      0.548\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/79     20.5G   0.02173   0.01705         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.797      0.707      0.753      0.593\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/79     20.5G   0.02125   0.01659         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.845      0.681      0.743      0.587\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/79     20.5G   0.02098   0.01631         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.771      0.699      0.743      0.598\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/79     20.5G    0.0199   0.01579         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.811      0.727      0.774      0.619\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/79     20.5G   0.01965   0.01558         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.778      0.723      0.767      0.617\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/79     20.5G   0.01938   0.01545         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.838      0.722      0.779      0.627\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     16/79     20.5G   0.01915   0.01519         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.834       0.67       0.73      0.597\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     17/79     20.5G    0.0189   0.01509         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.837      0.695      0.758      0.621\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     18/79     20.5G   0.01797   0.01454         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.857      0.682      0.755      0.615\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     19/79     20.5G   0.01821   0.01464         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.826      0.722      0.777      0.631\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     20/79     20.5G   0.01771    0.0144         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.861      0.693      0.767      0.624\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     21/79     20.5G   0.01753   0.01439         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.843      0.713       0.77       0.63\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     22/79     20.5G   0.01719   0.01414         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.829      0.709      0.766      0.628\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     23/79     20.5G   0.01719   0.01402         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.879      0.698      0.769      0.632\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     24/79     20.5G   0.01698   0.01389         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.864      0.715       0.78      0.638\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     25/79     20.5G   0.01653   0.01365         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@^C\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 636, in <module>\n",
      "    main(opt)\n",
      "  File \"train.py\", line 533, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"train.py\", line 367, in train\n",
      "    results, maps, _ = val.run(data_dict,\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/app/_data/yolov5_f2/val.py\", line 193, in run\n",
      "    dt[1] += time_sync() - t2\n",
      "  File \"/app/_data/yolov5_f2/utils/torch_utils.py\", line 91, in time_sync\n",
      "    torch.cuda.synchronize()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py\", line 446, in synchronize\n",
      "    return torch._C._cuda_synchronize()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img {IMG_SIZE} \\\n",
    "                --batch 4\\\n",
    "                --epochs 80 \\\n",
    "                --data {data_yaml_path} \\\n",
    "                --weights {WEIGHTS} \\\n",
    "                --name {NAME} \\\n",
    "                --hyp data/hyps/hyp.aug_custom.yaml \\\n",
    "                --single-cls \\\n",
    "                --patience 10 \\\n",
    "                --rect \\\n",
    "                --workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da4ec0a7-c855-4ded-95ac-74ef01d54a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/app/_data/yolov5/runs/train/yolov5m6_val2/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val2/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val110/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val110/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5s6_val29/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5s6_val29/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val2/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val2/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val23/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val23/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val1/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val1/weights/best.pt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"/app/_data/yolov5_f2/runs/train/*/weights/*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1d46550-29b7-4da9-ba35-22f762590794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in glob.glob('/app/_data/yolov5_f2/runs/train/*/'):\n",
    "#     shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ade69f-ca1e-40be-9991-5c4a252f3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R /app/_data/yolov5 /app/_data/YOLOv5_kaggle/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a624748-6aae-4a41-b3d6-132afdf58ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"/app/_data/YOLOv5_kaggle/yolov5_f2/.ipynb_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "281545d9-c8e1-40fd-9507-0d938a433ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/YOLOv5Weights/yolov5l6_val110.pt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(\n",
    "    \"/app/_data/yolov5_f2/runs/train/yolov5l6_val110/weights/best.pt\",\n",
    "    \"/app/_data/YOLOv5Weights/yolov5l6_val110.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd71a44-6192-4802-8c23-3b81d9c5fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(\"/app/_data/YOLOv5Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2bab4ef-dff9-4ebf-81d3-3e1082477479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t.gitignore\t\t data\t\t   setup.cfg\n",
      "..\t\t.pre-commit-config.yaml  detect.py\t   train.py\n",
      ".dockerignore\tCONTRIBUTING.md\t\t export.py\t   tutorial.ipynb\n",
      ".git\t\tDockerfile\t\t hubconf.py\t   utils\n",
      ".gitattributes\tLICENSE\t\t\t models\t\t   val.py\n",
      ".github\t\tREADME.md\t\t requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -a /app/_data/YOLOv5_kaggle/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c27d84a4-7cea-4b83-940d-f89da34cfd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = (\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "    )\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, \"int16\")\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n] = tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, \"int16\")\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8970b851-0714-4d1e-96da-fb093b41bb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"r\") as f:\n",
    "    res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b05b28d-e9b6-4045-8f21-bcd70e6639f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF_PART = \"/app/_data/tensorflow-great-barrier-reef/train.csv\"\n",
    "df = pd.read_csv(TRAIN_DF_PART)\n",
    "df[\"img_path\"] = (\n",
    "    \"/app/_data/tensorflow-great-barrier-reef/train_images/video_\"\n",
    "    + df.video_id.astype(\"str\")\n",
    "    + \"/\"\n",
    "    + df.video_frame.astype(\"str\")\n",
    "    + \".jpg\"\n",
    ")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"len_annotation\"] = df[\"annotations\"].str.len()\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"-\", \"_\", regex=True)\n",
    "df[\"new_img_path\"] = f\"/app/_data/{IMAGE_FOLDER}/\" + df[\"image_id\"] + \".jpg\"\n",
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "df[\"no_label\"] = df[\"len_annotation\"].apply(lambda x: True if x == 0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d25e6f47-ac37-4ee3-b787-4f409390b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df[df['video_id']==2].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30fa3d62-2065-4e65-9b27-d0001111c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"annotations\"] = df_test[\"annotations\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9bfe9161-53fa-49ee-bf83-2ef8930a4dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "Adding AutoShape... \n",
      "100% 8561/8561 [28:05<00:00,  5.08it/s]\n"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "path = f'/app/_data/yolov5_f2/runs/train/{NAME}/weights/best.pt'\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5_f2\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "# chose validation set\n",
    "df_test = val.copy()\n",
    "# computing f2 score\n",
    "for ix in tqdm(df_test.index.tolist()):\n",
    "    img = np.array(Image.open(df_test.loc[ix, \"img_path\"]))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    prediction = prediction[prediction[:, 4] > 0.1]\n",
    "    gt = np.array([list(x.values()) for x in df_test.loc[ix, \"annotations\"]])\n",
    "    if gt.size:\n",
    "        gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "        gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "    for n, c_th in enumerate(conf_thres):\n",
    "        TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "        res[n, 0, :] += TP\n",
    "        res[n, 1, :] += FP\n",
    "        res[n, 2, :] += FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef2ee32c-4232-4da4-8cee-d6376807745a",
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)\n",
    "if path not in res_dict:\n",
    "    res_dict[path] = {\n",
    "        IMG_SIZE: {\n",
    "            \"best\": [\n",
    "                np.round(conf_thres[np.argmax(F2)], 2),\n",
    "                np.round(np.max(F2), 4),\n",
    "            ],\n",
    "            \"all\": list(np.round(F2, 4)),\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    res_dict[path][IMG_SIZE] = {\n",
    "        \"best\": [\n",
    "            np.round(conf_thres[np.argmax(F2)], 2),\n",
    "            np.round(np.max(F2), 4),\n",
    "        ],\n",
    "        \"all\": list(np.round(F2, 4)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6058aaff-9335-4e79-8ee6-ada3f91e4ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best': [0.12, 0.6357],\n",
       " 'all': [0.6327,\n",
       "  0.6346,\n",
       "  0.6357,\n",
       "  0.635,\n",
       "  0.6344,\n",
       "  0.6347,\n",
       "  0.6339,\n",
       "  0.6341,\n",
       "  0.6327,\n",
       "  0.6324,\n",
       "  0.6304,\n",
       "  0.6302,\n",
       "  0.6295,\n",
       "  0.629,\n",
       "  0.6273,\n",
       "  0.6256,\n",
       "  0.6248,\n",
       "  0.6246,\n",
       "  0.6223,\n",
       "  0.6206,\n",
       "  0.6201,\n",
       "  0.6196,\n",
       "  0.6191,\n",
       "  0.6179,\n",
       "  0.6162,\n",
       "  0.6156,\n",
       "  0.6148,\n",
       "  0.6147,\n",
       "  0.6132,\n",
       "  0.6127,\n",
       "  0.612,\n",
       "  0.6094,\n",
       "  0.6075,\n",
       "  0.6066,\n",
       "  0.6044,\n",
       "  0.6027,\n",
       "  0.6022,\n",
       "  0.6013,\n",
       "  0.5993,\n",
       "  0.5983,\n",
       "  0.5972,\n",
       "  0.5955,\n",
       "  0.594,\n",
       "  0.5928,\n",
       "  0.5911,\n",
       "  0.5892,\n",
       "  0.5884,\n",
       "  0.5852,\n",
       "  0.5805,\n",
       "  0.5791,\n",
       "  0.5747]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict[path][IMG_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d41eed-b952-4dfe-8ebe-a58748676805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f0592-3b52-4846-b37f-f0342620fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2c55a2-c43f-4439-82cb-583006b0016d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33faef76-cbe0-4563-85f8-e4eb52cb3ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb0016-dcd3-40e7-be18-9a379606786a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
