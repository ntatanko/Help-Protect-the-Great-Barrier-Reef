{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "decbde53-4a1b-461e-a19e-80da565da2e9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d7d23a-8962-4ce1-a386-fa26c29ab911",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_FOLDER = \"images\"\n",
    "LABEL_FOLDER = \"labels\"\n",
    "SEED = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2970c2b9-b0ca-4b13-a088-85e312bdee23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/app/_data/train_alb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e1e8c5-84ec-4f24-b90e-799c21642294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>img_path</th>\n",
       "      <th>len_annotation</th>\n",
       "      <th>new_img_path</th>\n",
       "      <th>label</th>\n",
       "      <th>width_max</th>\n",
       "      <th>width_min</th>\n",
       "      <th>height_max</th>\n",
       "      <th>height_min</th>\n",
       "      <th>width_mean</th>\n",
       "      <th>height_mean</th>\n",
       "      <th>bbox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0_0</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_0.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_1.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_2.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0_3</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_3.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0_4</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>0</td>\n",
       "      <td>/app/_data/images/0_4.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38230</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10631</td>\n",
       "      <td>2859</td>\n",
       "      <td>2_10631_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10631_1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[51, 643, 44, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38231</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10631</td>\n",
       "      <td>2859</td>\n",
       "      <td>2_10631_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10631_2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>44.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[51, 643, 44, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38232</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2_10632_0</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10632_0.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[38, 681, 45, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38233</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2_10632_1</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10632_1.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[38, 681, 45, 37]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38234</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>10632</td>\n",
       "      <td>2860</td>\n",
       "      <td>2_10632_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>/app/_data/tensorflow-great-barrier-reef/train...</td>\n",
       "      <td>1</td>\n",
       "      <td>/app/_data/augmented/images/2_10632_2.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>[[38, 681, 45, 37]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38235 rows √ó 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id  sequence  video_frame  sequence_frame   image_id annotations  \\\n",
       "0             0     40258            0               0        0_0          []   \n",
       "1             0     40258            1               1        0_1          []   \n",
       "2             0     40258            2               2        0_2          []   \n",
       "3             0     40258            3               3        0_3          []   \n",
       "4             0     40258            4               4        0_4          []   \n",
       "...         ...       ...          ...             ...        ...         ...   \n",
       "38230         2     29859        10631            2859  2_10631_1          []   \n",
       "38231         2     29859        10631            2859  2_10631_2          []   \n",
       "38232         2     29859        10632            2860  2_10632_0          []   \n",
       "38233         2     29859        10632            2860  2_10632_1          []   \n",
       "38234         2     29859        10632            2860  2_10632_2          []   \n",
       "\n",
       "                                                img_path  len_annotation  \\\n",
       "0      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "1      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "2      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "3      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "4      /app/_data/tensorflow-great-barrier-reef/train...               0   \n",
       "...                                                  ...             ...   \n",
       "38230  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38231  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38232  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38233  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "38234  /app/_data/tensorflow-great-barrier-reef/train...               1   \n",
       "\n",
       "                                    new_img_path  label  width_max  width_min  \\\n",
       "0                      /app/_data/images/0_0.jpg      0        NaN        NaN   \n",
       "1                      /app/_data/images/0_1.jpg      0        NaN        NaN   \n",
       "2                      /app/_data/images/0_2.jpg      0        NaN        NaN   \n",
       "3                      /app/_data/images/0_3.jpg      0        NaN        NaN   \n",
       "4                      /app/_data/images/0_4.jpg      0        NaN        NaN   \n",
       "...                                          ...    ...        ...        ...   \n",
       "38230  /app/_data/augmented/images/2_10631_1.jpg      1       44.0       44.0   \n",
       "38231  /app/_data/augmented/images/2_10631_2.jpg      1       44.0       44.0   \n",
       "38232  /app/_data/augmented/images/2_10632_0.jpg      1       45.0       45.0   \n",
       "38233  /app/_data/augmented/images/2_10632_1.jpg      1       45.0       45.0   \n",
       "38234  /app/_data/augmented/images/2_10632_2.jpg      1       45.0       45.0   \n",
       "\n",
       "       height_max  height_min  width_mean  height_mean                 bbox  \n",
       "0             NaN         NaN         NaN          NaN                  NaN  \n",
       "1             NaN         NaN         NaN          NaN                  NaN  \n",
       "2             NaN         NaN         NaN          NaN                  NaN  \n",
       "3             NaN         NaN         NaN          NaN                  NaN  \n",
       "4             NaN         NaN         NaN          NaN                  NaN  \n",
       "...           ...         ...         ...          ...                  ...  \n",
       "38230        37.0        37.0        44.0         37.0  [[51, 643, 44, 37]]  \n",
       "38231        37.0        37.0        44.0         37.0  [[51, 643, 44, 37]]  \n",
       "38232        37.0        37.0        45.0         37.0  [[38, 681, 45, 37]]  \n",
       "38233        37.0        37.0        45.0         37.0  [[38, 681, 45, 37]]  \n",
       "38234        37.0        37.0        45.0         37.0  [[38, 681, 45, 37]]  \n",
       "\n",
       "[38235 rows x 17 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83d3620-7249-45d1-be36-b1eae5a7fdc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "df[\"no_label\"] = df[\"len_annotation\"].apply(lambda x: True if x == 0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dec12640-4a58-4e89-994a-50dc1c0b028a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label_change\"] = df[\"label\"] & df[\"no_label\"].shift(1) & df[\"no_label\"].shift(\n",
    "    2\n",
    ") | df[\"no_label\"] & df[\"label\"].shift(1) & df[\"label\"].shift(2)\n",
    "df[\"sequense_change\"] = df[\"sequence\"] != df[\"sequence\"].shift(1)\n",
    "df[\"start_subseq\"] = df[\"sequense_change\"] | df[\"label_change\"]\n",
    "df.loc[df.index[-1], \"start_subseq\"] = True\n",
    "df[\"start_subseq\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b5789f0-b515-451f-b9a4-3e5d902118c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = 0\n",
    "for subsequence_id, end_idx in enumerate(df[df[\"start_subseq\"]].index):\n",
    "    df.loc[start_idx:end_idx, \"subsequence_id\"] = subsequence_id\n",
    "    start_idx = end_idx\n",
    "\n",
    "df[\"subsequence_id\"] = df[\"subsequence_id\"].astype(int)\n",
    "df[\"subsequence_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e747db3-81b8-4210-8832-a4a5b6970c87",
   "metadata": {},
   "source": [
    "## KFold split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76ac1f8-87df-4ee8-b7c4-ddfc403b4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = GroupKFold(n_splits=4)\n",
    "# list_train_ids = []\n",
    "# list_val_ids= []\n",
    "# for train_idx, val_idx in (\n",
    "#     kf.split(train_df, y=train_df.len_annotation, groups=train_df.sequence)\n",
    "# ):\n",
    "#     list_train_ids.append(train_idx)\n",
    "#     list_val_ids.append(val_idx)\n",
    "#     print(train_df.loc[val_idx, \"len_annotation\"].values.sum(), train_df.loc[train_idx, \"len_annotation\"].values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0d4167f-6ffe-4f61-a64e-49235390d8a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    37735\n",
       "label             16949\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    9781\n",
       "label             2704\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    3.857990\n",
       "label             6.268121\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_ID = 2\n",
    "train = pd.concat(\n",
    "    [\n",
    "        df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\"),\n",
    "        df.query(\"video_id!=@VIDEO_ID and len_annotation==0\").sample(\n",
    "            int(df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\").shape[0] * 0.03)\n",
    "        ),\n",
    "    ]\n",
    ").sample(frac=1)\n",
    "val = df.query(\"video_id==@VIDEO_ID\").sample(frac=1)\n",
    "train[[\"len_annotation\", \"label\"]].sum()\n",
    "val[[\"len_annotation\", \"label\"]].sum()\n",
    "train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8460943a-9b4c-4383-9f96-97c1f3c84dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = (\n",
    "#     df.groupby(\"subsequence_id\")\n",
    "#     .agg({\"label\": \"max\", \"len_annotation\": \"sum\", \"video_frame\": \"count\"})\n",
    "#     .astype(int)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# n_splits = 10\n",
    "# y=df_split[\"label\"]\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# for fold_id, (train_idx, val_idx) in enumerate(\n",
    "#     skf.split(df_split[\"subsequence_id\"], y=y)\n",
    "# ):\n",
    "#     subseq_val_idx = df_split[\"subsequence_id\"].iloc[val_idx]\n",
    "#     df.loc[df[\"subsequence_id\"].isin(subseq_val_idx), \"fold\"] = fold_id\n",
    "\n",
    "# df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "# for fold in range(10):\n",
    "#     print(f\"\\nFold {fold}\")\n",
    "#     df.query(\"fold != @fold\")[[\"len_annotation\", \"label\"]].sum() / df.query(\n",
    "#         \"fold == @fold\"\n",
    "#     )[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02fd4978-53b4-4ad9-ae8c-929ebbc4b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD = 3\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"fold != @KFOLD and len_annotation!=0\"),\n",
    "#         df.query(\"fold != @KFOLD and len_annotation==0\").sample(\n",
    "#             int(df.query(\"fold != @KFOLD and len_annotation!=0\").shape[0] * 0.07)\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac=1)\n",
    "# val = df.query(\"fold == @KFOLD\").sample(frac=1)\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e70001-085d-48f0-8293-3a80d2c61964",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = train.index.tolist()\n",
    "val_ids = val.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eef53eb-0b36-4561-bb70-5967da2910da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5/data/reef_data_aug_val2.yaml'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/app/_data/val_aug_37_val2.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt =  f\"/app/_data/train_aug_{SEED}_val{VIDEO_ID}.txt\"\n",
    "val_txt = train_txt.replace('train_', 'val_')\n",
    "data_yaml_path = f'/app/_data/yolov5/data/reef_data_aug_val{VIDEO_ID}.yaml'\n",
    "data_yaml_path\n",
    "val_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e458f07f-b2b0-40fe-b502-83a968d86d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = df.loc[train_ids, \"new_img_path\"].tolist()\n",
    "val_img_path = df.loc[val_ids, \"new_img_path\"].tolist()\n",
    "np.savetxt(\n",
    "    train_txt,\n",
    "    train_img_path,\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "np.savetxt(val_txt, val_img_path, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dea2368-7385-4854-8d29-9d304fe8220b",
   "metadata": {},
   "source": [
    "## Custimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83a70a6b-161c-4950-b224-f0203dac98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5f1fa1-800f-4757-a46f-88423f5fe6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate {data_yaml_path}\n",
    "\n",
    "train: {train_txt} # training directory\n",
    "val: {val_txt} # validation directory\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: ['starfish']  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f40d3fd-5948-44d1-9aec-5773facfaaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: /app/_data/train_aug_37_val2.txt # training directory\n",
      "val: /app/_data/val_aug_37_val2.txt # validation directory\n",
      "\n",
      "# Classes\n",
      "nc: 1  # number of classes\n",
      "names: ['starfish']  # class names\n"
     ]
    }
   ],
   "source": [
    "!cat {data_yaml_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cd50d78-04f1-41d7-ab1d-63e073178ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
      "# Hyperparameters for COCO training from scratch\n",
      "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
      "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
      "\n",
      "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
      "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
      "momentum: 0.937  # SGD momentum/Adam beta1\n",
      "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
      "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
      "warmup_momentum: 0.8  # warmup initial momentum\n",
      "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
      "box: 0.05  # box loss gain\n",
      "cls: 0.5  # cls loss gain\n",
      "cls_pw: 1.0  # cls BCELoss positive_weight\n",
      "obj: 1.0  # obj loss gain (scale with pixels)\n",
      "obj_pw: 1.0  # obj BCELoss positive_weight\n",
      "iou_t: 0.20  # IoU training threshold\n",
      "anchor_t: 4.0  # anchor-multiple threshold\n",
      "# anchors: 3  # anchors per output layer (0 to ignore)\n",
      "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
      "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
      "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
      "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
      "degrees: 0.0  # image rotation (+/- deg)\n",
      "translate: 0.1  # image translation (+/- fraction)\n",
      "scale: 0.5  # image scale (+/- gain)\n",
      "shear: 0.0  # image shear (+/- deg)\n",
      "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
      "flipud: 0.0  # image flip up-down (probability)\n",
      "fliplr: 0.5  # image flip left-right (probability)\n",
      "mosaic: 1.0  # image mosaic (probability)\n",
      "mixup: 0.0  # image mixup (probability)\n",
      "copy_paste: 0.0  # segment copy-paste (probability)\n"
     ]
    }
   ],
   "source": [
    "!cat /app/_data/yolov5/data/hyps/hyp.scratch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d703fc0-49f8-408d-9f7d-a13e9d5767d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate /app/_data/yolov5/data/hyps/hyp.aug_custom.yaml\n",
    "# YOLOv5 üöÄ by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "lr0: 0.005  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
    "momentum: 0.937  # SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8  # warmup initial momentum\n",
    "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
    "box: 0.05  # box loss gain\n",
    "cls: 0.5  # cls loss gain\n",
    "cls_pw: 1.0  # cls BCELoss positive_weight\n",
    "obj: 1.0  # obj loss gain (scale with pixels)\n",
    "obj_pw: 1.0  # obj BCELoss positive_weight\n",
    "iou_t: 0.20  # IoU training threshold\n",
    "anchor_t: 4.0  # anchor-multiple threshold\n",
    "# anchors: 3  # anchors per output layer (0 to ignore)\n",
    "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
    "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.3  # image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.3  # image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0  # image rotation (+/- deg)\n",
    "translate: 0.1  # image translation (+/- fraction)\n",
    "scale: 0.1  # image scale (+/- gain)\n",
    "shear: 0.0  # image shear (+/- deg)\n",
    "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.1  # image flip up-down (probability)\n",
    "fliplr: 0.5  # image flip left-right (probability)\n",
    "mosaic: 1.0  # image mosaic (probability)\n",
    "mixup: 0.0  # image mixup (probability)\n",
    "copy_paste: 0.0  # segment copy-paste (probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55a22a72-2c12-4518-9841-020910838e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb\n",
    "clear_output()\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2419f31-9c26-4c37-9d2e-b5d97480d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app/_data/yolov5/\n",
    "!pip install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e97c0a4-c9f6-43b1-ac93-a9760ccf10bf",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc096e27-cec5-44af-aff1-aba140d260d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5l6_2560_val2_rect_aug'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS = \"yolov5l6.pt\"\n",
    "IMG_SIZE = 2560\n",
    "NAME = f\"{WEIGHTS[:-3]}_{IMG_SIZE}_val{VIDEO_ID}_rect_aug\"\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "881688e4-6cd7-43fc-b322-498f83f79f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.rmtree('/app/_data/yolov5/runs/train/yolov5l6_2560_val2_rect_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6cc22cb-b57a-4973-9141-6bd4f11f5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l6.pt, cfg=, data=/app/_data/yolov5/data/reef_data_aug_val2.yaml, hyp=data/hyps/hyp.aug_custom.yaml, epochs=80, batch_size=4, imgsz=2560, rect=True, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=True, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=yolov5l6_2560_val2_rect_aug, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=10, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5 üöÄ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.005, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.3, hsv_v=0.3, degrees=0.0, translate=0.1, scale=0.1, shear=0.0, perspective=0.0, flipud=0.1, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myolov5l6_2560_val2_rect_aug\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5/runs/1egyrbxm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /app/_data/yolov5/wandb/run-20220202_150235-1egyrbxm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   3540480  models.common.Conv                      [512, 768, 3, 2]              \n",
      "  8                -1  3   5611008  models.common.C3                        [768, 768, 3]                 \n",
      "  9                -1  1   7079936  models.common.Conv                      [768, 1024, 3, 2]             \n",
      " 10                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      " 11                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 12                -1  1    787968  models.common.Conv                      [1024, 768, 1, 1]             \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  3   6200832  models.common.C3                        [1536, 768, 3, False]         \n",
      " 16                -1  1    394240  models.common.Conv                      [768, 512, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 24                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 27                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  3   5807616  models.common.C3                        [1024, 768, 3, False]         \n",
      " 30                -1  1   5309952  models.common.Conv                      [768, 768, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  3  10496000  models.common.C3                        [1536, 1024, 3, False]        \n",
      " 33  [23, 26, 29, 32]  1     46152  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [256, 512, 768, 1024]]\n",
      "Model Summary: 607 layers, 76162504 parameters, 76162504 gradients, 110.2 GFLOPs\n",
      "\n",
      "Transferred 787/795 items from yolov5l6.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight (no decay), 135 weight, 135 bias\n",
      "WARNING: --rect is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 3)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 3)), ToGray(always_apply=False, p=0.01), RandomBrightnessContrast(always_apply=False, p=0.1, brightness_limit=(-0.05, 0.05), contrast_limit=(-0.05, 0.05), brightness_by_max=True), RandomGamma(always_apply=False, p=0.05, gamma_limit=(80, 120), eps=None), ImageCompression(always_apply=False, p=0.05, quality_lower=95, quality_upper=100, compression_type=0), Sharpen(always_apply=False, p=0.05, alpha=(0.2, 0.3), lightness=(0.5, 1.0)), HueSaturationValue(always_apply=False, p=0.01, hue_shift_limit=(-5, 5), sat_shift_limit=(-5, 5), val_shift_limit=(-5, 5)), RGBShift(always_apply=False, p=0.01, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)), Flip(always_apply=False, p=0.01), ShiftScaleRotate(always_apply=False, p=0.05, shift_limit_x=(-0.2, 0.2), shift_limit_y=(-0.2, 0.2), scale_limit=(0.10000000000000009, 0.19999999999999996), rotate_limit=(10, 45), interpolation=1, border_mode=4, value=None, mask_value=None), RandomRain(always_apply=False, p=0.01, slant_lower=-10, slant_upper=10, drop_length=20, drop_width=1, drop_color=(200, 200, 200), blur_value=7, brightness_coefficient=0.6, rain_type='drizzle')\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/app/_data/train_aug_37_val2' images and labels...16949 found, \u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /app/_data/train_aug_37_val2.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/app/_data/val_aug_37_val2' images and labels...2704 found, 7884 \u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /app/_data/val_aug_37_val2.cache\n",
      "Plotting labels to runs/train/yolov5l6_2560_val2_rect_aug/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.48 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Image sizes 2560 train, 2560 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov5l6_2560_val2_rect_aug\u001b[0m\n",
      "Starting training for 80 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/79     21.3G   0.04537   0.04915         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.511      0.599      0.608      0.391\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/79     20.6G   0.03338   0.03124         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781       0.64      0.648      0.675      0.483\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/79     20.6G   0.03274   0.02863         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.577      0.522      0.547       0.42\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/79     20.6G   0.02982   0.02662         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.624      0.657      0.677      0.529\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/79     20.6G   0.02631   0.02253         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.738      0.668      0.707      0.545\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/79     20.6G   0.02416   0.02013         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.709       0.71      0.753      0.585\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/79     20.6G   0.02279   0.01871         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.714      0.659      0.696      0.542\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/79     20.6G   0.02159   0.01795         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.739      0.669      0.711      0.559\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/79     20.6G   0.02087   0.01745         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.787      0.679      0.729      0.581\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/79     20.6G   0.01989   0.01688         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.778      0.653      0.707      0.574\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/79     20.6G   0.01934   0.01669         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.755      0.617      0.674      0.546\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/79     20.6G   0.01884   0.01641         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.773      0.664      0.717      0.544\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/79     20.6G   0.01814   0.01594         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.858      0.646      0.715      0.551\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/79     20.6G   0.01774   0.01568         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.863      0.621      0.695      0.554\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/79     20.6G   0.01735   0.01534         0         0      2560: 100%|‚ñà‚ñà‚ñà    14/79     20.6G   0.01734    0.0165         0         4      2560:  79%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.867      0.636      0.714      0.571\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     15/79     20.6G   0.01696   0.01515         0         0      2560: 100%|‚ñà‚ñà‚ñà\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.875      0.634      0.715      0.581\n",
      "Stopping training early as no improvement observed in last 10 epochs. Best results observed at epoch 5, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n",
      "\n",
      "16 epochs completed in 23.211 hours.\n",
      "Optimizer stripped from runs/train/yolov5l6_2560_val2_rect_aug/weights/last.pt, 154.8MB\n",
      "Optimizer stripped from runs/train/yolov5l6_2560_val2_rect_aug/weights/best.pt, 154.8MB\n",
      "\n",
      "Validating runs/train/yolov5l6_2560_val2_rect_aug/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all      10588       9781      0.709       0.71      0.753      0.585\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 426... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÉ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÅ‚ñÑ‚ñÇ‚ñÜ‚ñá‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñÖ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÑ‚ñÜ‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñÖ‚ñÉ‚ñÉ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñÑ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÅ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             best/epoch 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best/mAP_0.5 0.7527\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      best/mAP_0.5:0.95 0.58544\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         best/precision 0.70897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            best/recall 0.71036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.75271\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.58542\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.70933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.71036\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.01696\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.01515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.00877\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.01238\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 81 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33myolov5l6_2560_val2_rect_aug\u001b[0m: \u001b[34mhttps://wandb.ai/tatanko/YOLOv5/runs/1egyrbxm\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220202_150235-1egyrbxm/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "Results saved to \u001b[1mruns/train/yolov5l6_2560_val2_rect_aug\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img {IMG_SIZE} \\\n",
    "                --batch 4\\\n",
    "                --epochs 80 \\\n",
    "                --data {data_yaml_path} \\\n",
    "                --weights {WEIGHTS} \\\n",
    "                --name {NAME} \\\n",
    "                --hyp data/hyps/hyp.aug_custom.yaml \\\n",
    "                --single-cls \\\n",
    "                --patience 10 \\\n",
    "                --rect \\\n",
    "                --workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e64167a5-eda5-45d4-85f8-37745d9a0a54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/app/_data/yolov5/runs/train/yolov5m6_val2/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val2/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val110/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val110/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5s6_val29/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5s6_val29/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val2/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5l6_val2/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m_val1_1/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val23/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val23/weights/best.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val1/weights/last.pt',\n",
       " '/app/_data/yolov5/runs/train/yolov5m6_val1/weights/best.pt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(\"/app/_data/yolov5/runs/train/*/weights/*.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a06b22d3-c469-4f3b-a9b5-8cffae0a3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for path in glob.glob('/app/_data/yolov5/runs/train/*/'):\n",
    "#     shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "587fca1a-3a33-4559-9f9f-e7ab0b5bdee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -R /app/_data/yolov5 /app/_data/YOLOv5_kaggle/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c598f052-539e-420b-bf51-d1e4b0139588",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"/app/_data/YOLOv5_kaggle/yolov5/.ipynb_checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ab3495-08e5-4a47-8fcd-55633478c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/YOLOv5Weights/yolov5l6_val110.pt'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy(\n",
    "    \"/app/_data/yolov5/runs/train/yolov5l6_val110/weights/best.pt\",\n",
    "    \"/app/_data/YOLOv5Weights/yolov5l6_val110.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486b4a9-ba6b-4c50-8893-39bf3ae0963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.copy(\"/app/_data/YOLOv5Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b21e42-44bc-44b4-a29f-e22ee6be9e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\t\t.gitignore\t\t data\t\t   setup.cfg\n",
      "..\t\t.pre-commit-config.yaml  detect.py\t   train.py\n",
      ".dockerignore\tCONTRIBUTING.md\t\t export.py\t   tutorial.ipynb\n",
      ".git\t\tDockerfile\t\t hubconf.py\t   utils\n",
      ".gitattributes\tLICENSE\t\t\t models\t\t   val.py\n",
      ".github\t\tREADME.md\t\t requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls -a /app/_data/YOLOv5_kaggle/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1954391-8a19-47eb-9614-6cf372e1bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = (\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "    )\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, \"int16\")\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n] = tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, \"int16\")\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a4cbb9f-7cfd-4214-a900-259cf51c0236",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"r\") as f:\n",
    "    res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07d66e47-a99c-4265-98b7-cba0e8ba9691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>...</th>\n",
       "      <th>no_label</th>\n",
       "      <th>label_change</th>\n",
       "      <th>sequense_change</th>\n",
       "      <th>start_subseq</th>\n",
       "      <th>subsequence_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37567</th>\n",
       "      <td>2</td>\n",
       "      <td>22643</td>\n",
       "      <td>5946</td>\n",
       "      <td>583</td>\n",
       "      <td>2_5946_2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17702</th>\n",
       "      <td>2</td>\n",
       "      <td>37114</td>\n",
       "      <td>2762</td>\n",
       "      <td>2762</td>\n",
       "      <td>2_2762</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21473</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>8732</td>\n",
       "      <td>960</td>\n",
       "      <td>2_8732</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17602</th>\n",
       "      <td>2</td>\n",
       "      <td>37114</td>\n",
       "      <td>2662</td>\n",
       "      <td>2662</td>\n",
       "      <td>2_2662</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37199</th>\n",
       "      <td>2</td>\n",
       "      <td>22643</td>\n",
       "      <td>5824</td>\n",
       "      <td>461</td>\n",
       "      <td>2_5824_0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36791</th>\n",
       "      <td>2</td>\n",
       "      <td>22643</td>\n",
       "      <td>5688</td>\n",
       "      <td>325</td>\n",
       "      <td>2_5688_0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21058</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>8317</td>\n",
       "      <td>545</td>\n",
       "      <td>2_8317</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16334</th>\n",
       "      <td>2</td>\n",
       "      <td>37114</td>\n",
       "      <td>1394</td>\n",
       "      <td>1394</td>\n",
       "      <td>2_1394</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38174</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>8864</td>\n",
       "      <td>1092</td>\n",
       "      <td>2_8864_2</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20552</th>\n",
       "      <td>2</td>\n",
       "      <td>29859</td>\n",
       "      <td>7811</td>\n",
       "      <td>39</td>\n",
       "      <td>2_7811</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10588 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       video_id  sequence  video_frame  sequence_frame  image_id  ...  \\\n",
       "37567         2     22643         5946             583  2_5946_2  ...   \n",
       "17702         2     37114         2762            2762    2_2762  ...   \n",
       "21473         2     29859         8732             960    2_8732  ...   \n",
       "17602         2     37114         2662            2662    2_2662  ...   \n",
       "37199         2     22643         5824             461  2_5824_0  ...   \n",
       "...         ...       ...          ...             ...       ...  ...   \n",
       "36791         2     22643         5688             325  2_5688_0  ...   \n",
       "21058         2     29859         8317             545    2_8317  ...   \n",
       "16334         2     37114         1394            1394    2_1394  ...   \n",
       "38174         2     29859         8864            1092  2_8864_2  ...   \n",
       "20552         2     29859         7811              39    2_7811  ...   \n",
       "\n",
       "      no_label label_change  sequense_change start_subseq  subsequence_id  \n",
       "37567    False        False            False        False             153  \n",
       "17702     True        False            False        False             113  \n",
       "21473     True        False            False        False             133  \n",
       "17602     True        False            False        False             113  \n",
       "37199    False        False            False        False             153  \n",
       "...        ...          ...              ...          ...             ...  \n",
       "36791    False        False            False        False             153  \n",
       "21058     True        False            False        False             133  \n",
       "16334     True        False            False        False             113  \n",
       "38174    False        False            False        False             154  \n",
       "20552     True        False            False        False             131  \n",
       "\n",
       "[10588 rows x 22 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0916296b-6f99-456b-b5e6-59190f338577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 üöÄ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "Adding AutoShape... \n",
      "  0% 0/10588 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_345/3047035183.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxywh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_345/3047035183.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxywh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"annotations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "path = f'/app/_data/yolov5/runs/train/{NAME}/weights/best.pt'\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "# chose validation set\n",
    "df_test = val.copy()\n",
    "# computing f2 score\n",
    "for ix in tqdm(df_test.index.tolist()):\n",
    "    img = np.array(Image.open(df_test.loc[ix, \"img_path\"]))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    prediction = prediction[prediction[:, 4] > 0.1]\n",
    "    gt = np.array([list(x.values()) for x in df_test.loc[ix, \"annotations\"]])\n",
    "    if gt.size:\n",
    "        gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "        gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "    for n, c_th in enumerate(conf_thres):\n",
    "        TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "        res[n, 0, :] += TP\n",
    "        res[n, 1, :] += FP\n",
    "        res[n, 2, :] += FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3863437d-3876-4b07-b80b-39abd24ebb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)\n",
    "if path not in res_dict:\n",
    "    res_dict[path] = {\n",
    "        IMG_SIZE: {\n",
    "            \"best\": [\n",
    "                np.round(conf_thres[np.argmax(F2)], 2),\n",
    "                np.round(np.max(F2), 4),\n",
    "            ],\n",
    "            \"all\": list(np.round(F2, 4)),\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    res_dict[path][IMG_SIZE] = {\n",
    "        \"best\": [\n",
    "            np.round(conf_thres[np.argmax(F2)], 2),\n",
    "            np.round(np.max(F2), 4),\n",
    "        ],\n",
    "        \"all\": list(np.round(F2, 4)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "373154a3-3305-4ecb-8029-992090a75c72",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'/app/_data/yolov5/runs/train/yolov5l6_2560_val2_rect_aug/weights/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_345/1407970444.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: '/app/_data/yolov5/runs/train/yolov5l6_2560_val2_rect_aug/weights/best.pt'"
     ]
    }
   ],
   "source": [
    "res_dict[path][IMG_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4da62f-6e30-40b8-9ce5-ba39d4059f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac222066-9fe4-4861-a181-41975b33f036",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715a23b-0231-4a71-92bd-b824d8fb454c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499c1c93-841a-40c5-9f6f-d44bf427e447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d654bf0a-04d9-4f9b-85d4-7f5eea46f793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
