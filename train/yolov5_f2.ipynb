{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e15498f1-06eb-4cdf-b6b1-d4b7b1eadabc",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from random import sample\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import GroupKFold, KFold, StratifiedKFold\n",
    "from tqdm import tqdm\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torchvision\n",
    "from IPython.display import clear_output\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd139a3e-a97c-4b63-9312-678e96c707b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_DF_PART = \"/app/_data/tensorflow-great-barrier-reef/train.csv\"\n",
    "IMAGE_FOLDER = \"images\"\n",
    "LABEL_FOLDER = \"labels\"\n",
    "SEED = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6629510-d137-446b-9c18-df29f887be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/_data/sequences.json\", \"r\") as f:\n",
    "    seq_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54ff98e1-3cd2-481c-abc5-eb6ddfaa2466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(TRAIN_DF_PART)\n",
    "df[\"img_path\"] = (\n",
    "    \"/app/_data/tensorflow-great-barrier-reef/train_images/video_\"\n",
    "    + df.video_id.astype(\"str\")\n",
    "    + \"/\"\n",
    "    + df.video_frame.astype(\"str\")\n",
    "    + \".jpg\"\n",
    ")\n",
    "df[\"annotations\"] = df[\"annotations\"].apply(lambda x: ast.literal_eval(x))\n",
    "df[\"len_annotation\"] = df[\"annotations\"].str.len()\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"-\", \"_\", regex=True)\n",
    "df[\"new_img_path\"] = f\"/app/_data/{IMAGE_FOLDER}/\" + df[\"image_id\"] + \".jpg\"\n",
    "df[\"label\"] = df[\"len_annotation\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "df[\"no_label\"] = df[\"len_annotation\"].apply(lambda x: True if x == 0 else False)\n",
    "R = df[df[\"len_annotation\"] == 0].shape[0] / df[df[\"len_annotation\"] != 0].shape[0]\n",
    "df[\"label_change\"] = df[\"label\"] & df[\"no_label\"].shift(1) & df[\"no_label\"].shift(\n",
    "    2\n",
    ") | df[\"no_label\"] & df[\"label\"].shift(1) & df[\"label\"].shift(2)\n",
    "df[\"sequense_change\"] = df[\"sequence\"] != df[\"sequence\"].shift(1)\n",
    "df[\"start_subseq\"] = df[\"sequense_change\"] | df[\"label_change\"]\n",
    "df.loc[df.index[-1], \"start_subseq\"] = True\n",
    "df[\"start_subseq\"].sum()\n",
    "start_idx = 0\n",
    "for subsequence_id, end_idx in enumerate(df[df[\"start_subseq\"]].index):\n",
    "    df.loc[start_idx:end_idx, \"subsequence_id\"] = subsequence_id\n",
    "    start_idx = end_idx\n",
    "\n",
    "df[\"subsequence_id\"] = df[\"subsequence_id\"].astype(int)\n",
    "df[\"subsequence_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa92820-47f1-4de4-a3a6-cdbf66c3a7ef",
   "metadata": {},
   "source": [
    "# train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b2afa-3d15-4998-8a3b-13759e808a32",
   "metadata": {},
   "source": [
    "## random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53741bf6-c21e-439e-93ed-ba6e8e2d2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39% 772/2000 [00:13<00:21, 58.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    8.288056\n",
       "label             8.683071\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "4.188976377952756"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[10, 15, 19, 32, 59, 64, 71, 76, 86, 93, 115, 120, 125, 133, 137]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "ll = 0\n",
    "for i in tqdm(range(2000)):\n",
    "    n_val = np.random.randint(15, 25, 1)[0]\n",
    "    val_seq = sample(df.subsequence_id.unique().tolist(), n_val)\n",
    "    train_seq = list(set(df.subsequence_id.tolist()) - set(val_seq))\n",
    "    l = (\n",
    "        df.query(\"subsequence_id in @train_seq\")[\"label\"].sum()\n",
    "        / df.query(\"subsequence_id in @val_seq\")[\"label\"].sum()\n",
    "    )\n",
    "    ll = (\n",
    "        df.query(\"subsequence_id in @train_seq\")[\"len_annotation\"].sum()\n",
    "        / df.query(\"subsequence_id in @val_seq\")[\"len_annotation\"].sum()\n",
    "    )\n",
    "    r = (\n",
    "        df.query(\"subsequence_id in @val_seq and len_annotation == 0\").shape[0]\n",
    "        / df.query(\"subsequence_id in @val_seq and len_annotation != 0\").shape[0]\n",
    "    )\n",
    "    if 8 <= l <= 10 and 8 <= ll <= 10 and 3.5 <= r <= 4.3:\n",
    "        break\n",
    "\n",
    "df.query(\"subsequence_id in @train_seq\")[[\"len_annotation\", \"label\"]].sum() / df.query(\n",
    "    \"subsequence_id in @val_seq\"\n",
    ")[[\"len_annotation\", \"label\"]].sum()\n",
    "\n",
    "df.query(\"subsequence_id in @val_seq and len_annotation == 0\").shape[0] / df.query(\n",
    "    \"subsequence_id in @val_seq and len_annotation != 0\"\n",
    ").shape[0]\n",
    "\n",
    "sorted(val_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acc3b7a-e2ce-492c-a9c2-cfbf55e655d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "len_annotation    10617\n",
       "label              4411\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    1281\n",
       "label              508\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "len_annotation    8.288056\n",
       "label             8.683071\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KFOLD = \"rand\"\n",
    "train_idx = df.query(\"subsequence_id in @train_seq and len_annotation>0\").index.tolist()\n",
    "val_idx = df.query(\"subsequence_id in @val_seq\").index.tolist()\n",
    "train = df.loc[train_idx]\n",
    "val = df.loc[val_idx]\n",
    "train[[\"len_annotation\", \"label\"]].sum()\n",
    "val[[\"len_annotation\", \"label\"]].sum()\n",
    "train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4ad1b8-6d53-4aa2-a75c-7c0f40dbe016",
   "metadata": {},
   "source": [
    "# GroupKFold on subsequence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc49c87-cd4d-4062-a24d-dd5347b81793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = df.query('len_annotation>0').reset_index(drop=True)\n",
    "# kf = GroupKFold(n_splits=10)\n",
    "# list_train_ids = []\n",
    "# list_val_ids= []\n",
    "# for kfold, (train_idx, val_idx) in enumerate(\n",
    "#     kf.split(train_df, y=train_df.len_annotation, groups=train_df.subsequence_id)\n",
    "# ):\n",
    "#     list_train_ids.append(train_idx)\n",
    "#     list_val_ids.append(val_idx)\n",
    "#     print(kfold, '\\n',train_df.loc[train_idx, [\"len_annotation\", 'label']].sum()/ train_df.loc[val_idx, [\"len_annotation\", 'label']].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dc5245b-72d5-4798-b00c-d2a7ba40bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD = 6\n",
    "# train_idx = list_train_ids[KFOLD]\n",
    "# val_idx = list_val_ids[KFOLD]\n",
    "# train = train_df.loc[train_idx]\n",
    "# val = train_df.loc[val_idx]\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34884eae-1ce8-49b4-8392-226fab7046d3",
   "metadata": {},
   "source": [
    "## video_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49cfe90c-81c4-424e-9cca-5dd8efa3b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIDEO_ID = 2\n",
    "# ids = f'val{VIDEO_ID}'\n",
    "# train  = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\"),\n",
    "#         df.query(\"video_id!=@VIDEO_ID and len_annotation==0\").sample(\n",
    "#             int(\n",
    "#                 df.query(\"video_id!=@VIDEO_ID and len_annotation!=0\").shape[0]\n",
    "#                 * 0.07\n",
    "#             )\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac = 1)\n",
    "# val = df.query(\"video_id==@VIDEO_ID and len_annotation!=0\").sample(frac = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091a1d9-dc5b-44b1-b297-97987aead51e",
   "metadata": {},
   "source": [
    "## StratifiedKFold on subsequence_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa79579f-2284-4bca-8936-eaa885340307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = (\n",
    "#     df.groupby(\"subsequence_id\")\n",
    "#     .agg({\"label\": \"max\", \"len_annotation\": \"sum\", \"video_frame\": \"count\"})\n",
    "#     .astype(int)\n",
    "#     .reset_index()\n",
    "# )\n",
    "# n_splits = 10\n",
    "# y=df_split[\"label\"]\n",
    "# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# for fold_id, (train_idx, val_idx) in enumerate(\n",
    "#     skf.split(df_split[\"subsequence_id\"], y=y)\n",
    "# ):\n",
    "#     subseq_val_idx = df_split[\"subsequence_id\"].iloc[val_idx]\n",
    "#     df.loc[df[\"subsequence_id\"].isin(subseq_val_idx), \"fold\"] = fold_id\n",
    "\n",
    "# df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "# for fold in range(10):\n",
    "#     print(f\"\\nFold {fold}\")\n",
    "#     df.query(\"fold != @fold\")[[\"len_annotation\", \"label\"]].sum() / df.query(\n",
    "#         \"fold == @fold\"\n",
    "#     )[[\"len_annotation\", \"label\"]].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a23c518-6ca8-49e5-acba-09b9c774ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFOLD = 3\n",
    "# train = pd.concat(\n",
    "#     [\n",
    "#         df.query(\"fold != @KFOLD and len_annotation!=0\"),\n",
    "#         df.query(\"fold != @KFOLD and len_annotation==0\").sample(\n",
    "#             int(df.query(\"fold != @KFOLD and len_annotation!=0\").shape[0] * 0.07)\n",
    "#         ),\n",
    "#     ]\n",
    "# ).sample(frac=1)\n",
    "# val = df.query(\"fold == @KFOLD\").sample(frac=1)\n",
    "# train[[\"len_annotation\", \"label\"]].sum() / val[[\"len_annotation\", \"label\"]].sum()\n",
    "\n",
    "# train[[\"len_annotation\", \"label\"]].sum()\n",
    "\n",
    "# val[[\"len_annotation\", \"label\"]].sum()\n",
    "\n",
    "# train_ids = train.index.tolist()\n",
    "# val_ids = val.index.tolist()\n",
    "\n",
    "# len(train_ids), len(val_ids)\n",
    "\n",
    "# train_img_path = df.loc[train_ids, \"new_img_path\"].tolist()\n",
    "# val_img_path = df.loc[val_ids, \"new_img_path\"].tolist()\n",
    "# np.savetxt(\n",
    "#     f\"/app/_data/train_{SEED}_{KFOLD}.txt\",\n",
    "#     train_img_path,\n",
    "#     fmt=\"%s\",\n",
    "# )\n",
    "# np.savetxt(f\"/app/_data/val_{SEED}_{KFOLD}.txt\", val_img_path, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde01af6-2c91-454e-b8ee-beea56c2079e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/app/_data/yolov5_f2/data/reef_data_seq_id_rand_f2.yaml'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'/app/_data/val_37_seq_id_rand0_f2.txt'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_txt = f\"/app/_data/train_{SEED}_seq_id_{KFOLD}0_f2.txt\"\n",
    "val_txt = train_txt.replace(\"train_\", \"val_\")\n",
    "data_yaml_path = f\"/app/_data/yolov5_f2/data/reef_data_seq_id_{KFOLD}_f2.yaml\"\n",
    "data_yaml_path\n",
    "val_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13654395-177a-498f-a2f3-a6aad0b5df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_path = train[\"new_img_path\"].tolist()\n",
    "val_img_path = val[\"new_img_path\"].tolist()\n",
    "np.savetxt(\n",
    "    train_txt,\n",
    "    train_img_path,\n",
    "    fmt=\"%s\",\n",
    ")\n",
    "np.savetxt(val_txt, val_img_path, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29a240-f419-40f6-a5f1-d8d4b6d290b0",
   "metadata": {},
   "source": [
    "## Custimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7eacab5-f919-4bd7-b366-5b255a8c2e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_line_cell_magic\n",
    "\n",
    "\n",
    "@register_line_cell_magic\n",
    "def writetemplate(line, cell):\n",
    "    with open(line, \"w\") as f:\n",
    "        f.write(cell.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08dbb49-ba14-4e84-b623-56a3759afb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate {data_yaml_path}\n",
    "\n",
    "train: {train_txt}  # training directory\n",
    "val: {val_txt}  # validation directory\n",
    "\n",
    "# Classes\n",
    "nc: 1  # number of classes\n",
    "names: [\"starfish\"]  # class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62ab7b49-e49e-480c-b900-28c09f819455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train: /app/_data/train_37_seq_id_rand0_f2.txt  # training directory\n",
      "val: /app/_data/val_37_seq_id_rand0_f2.txt  # validation directory\n",
      "\n",
      "# Classes\n",
      "nc: 1  # number of classes\n",
      "names: [\"starfish\"]  # class names\n"
     ]
    }
   ],
   "source": [
    "!cat {data_yaml_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1406cf1-00f7-4aea-a882-6fd84c13b934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
      "# Hyperparameters for COCO training from scratch\n",
      "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
      "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
      "\n",
      "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
      "lrf: 0.1  # final OneCycleLR learning rate (lr0 * lrf)\n",
      "momentum: 0.937  # SGD momentum/Adam beta1\n",
      "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
      "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
      "warmup_momentum: 0.8  # warmup initial momentum\n",
      "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
      "box: 0.05  # box loss gain\n",
      "cls: 0.5  # cls loss gain\n",
      "cls_pw: 1.0  # cls BCELoss positive_weight\n",
      "obj: 1.0  # obj loss gain (scale with pixels)\n",
      "obj_pw: 1.0  # obj BCELoss positive_weight\n",
      "iou_t: 0.20  # IoU training threshold\n",
      "anchor_t: 4.0  # anchor-multiple threshold\n",
      "# anchors: 3  # anchors per output layer (0 to ignore)\n",
      "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
      "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
      "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
      "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
      "degrees: 0.0  # image rotation (+/- deg)\n",
      "translate: 0.1  # image translation (+/- fraction)\n",
      "scale: 0.5  # image scale (+/- gain)\n",
      "shear: 0.0  # image shear (+/- deg)\n",
      "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
      "flipud: 0.0  # image flip up-down (probability)\n",
      "fliplr: 0.5  # image flip left-right (probability)\n",
      "mosaic: 1.0  # image mosaic (probability)\n",
      "mixup: 0.0  # image mixup (probability)\n",
      "copy_paste: 0.0  # segment copy-paste (probability)\n"
     ]
    }
   ],
   "source": [
    "!cat /app/_data/yolov5_f2/data/hyps/hyp.scratch.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "905a615a-a1b7-41a4-9e0b-90f88906ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writetemplate /app/_data/yolov5_f2/data/hyps/hyp.custom.seq.yaml\n",
    "# YOLOv5 ðŸš€ by Ultralytics, GPL-3.0 license\n",
    "# Hyperparameters for COCO training from scratch\n",
    "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
    "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
    "\n",
    "\n",
    "lr0: 0.01\n",
    "lrf: 0.1\n",
    "momentum: 0.937\n",
    "weight_decay: 0.0005\n",
    "warmup_epochs: 3.0\n",
    "warmup_momentum: 0.8\n",
    "warmup_bias_lr: 0.1\n",
    "box: 0.05\n",
    "cls: 0.5\n",
    "cls_pw: 1.0\n",
    "obj: 1.0\n",
    "obj_pw: 1.0\n",
    "iou_t: 0.2\n",
    "anchor_t: 4.0\n",
    "fl_gamma: 0.0\n",
    "hsv_h: 0.015\n",
    "hsv_s: 0.7\n",
    "hsv_v: 0.4\n",
    "degrees: 0.0\n",
    "translate: 0.1\n",
    "scale: 0.5\n",
    "shear: 0.0\n",
    "perspective: 0.0\n",
    "flipud: 0.5\n",
    "fliplr: 0.5\n",
    "mosaic: 1.0\n",
    "mixup: 0.5\n",
    "copy_paste: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fb41f5-6a3b-4a05-8cd3-1837fe79c4b8",
   "metadata": {},
   "source": [
    "# yolov5 requirements and wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be66e067-aa35-4a37-9981-84328169f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install --upgrade wandb\n",
    "clear_output()\n",
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bf3cf21-e642-47ca-8960-b1d008e2d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app/_data/yolov5_f2/\n",
    "!pip install -r requirements.txt\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af20f4c-af16-40bb-a42e-8bd5fd1ef885",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f401301e-7ff5-44fc-a0ef-cb947c78c7cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yolov5l6_2880_seq_id_rand0_f2'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WEIGHTS = \"yolov5l6.pt\"\n",
    "IMG_SIZE = 2880\n",
    "NAME = f\"{WEIGHTS[:-3]}_{IMG_SIZE}_seq_id_{KFOLD}0_f2\"\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c058fb5-90e9-4a02-8399-63cc7c163f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtatanko\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l6.pt, cfg=, data=/app/_data/yolov5_f2/data/reef_data_seq_id_rand_f2.yaml, hyp=data/hyps/hyp.custom.seq.yaml, epochs=80, batch_size=2, imgsz=2880, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=True, optimizer=SGD, sync_bn=False, workers=0, project=runs/train, name=yolov5l6_2880_seq_id_rand0_f2, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=10, freeze=[0], save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (Docker image), for updates see https://github.com/ultralytics/yolov5\n",
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.5, fliplr=0.5, mosaic=1.0, mixup=0.5, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33myolov5l6_2880_seq_id_rand0_f2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tatanko/YOLOv5/runs/38rbf4kx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /app/_data/yolov5_f2/wandb/run-20220206_075349-38rbf4kx\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   3540480  models.common.Conv                      [512, 768, 3, 2]              \n",
      "  8                -1  3   5611008  models.common.C3                        [768, 768, 3]                 \n",
      "  9                -1  1   7079936  models.common.Conv                      [768, 1024, 3, 2]             \n",
      " 10                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      " 11                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 12                -1  1    787968  models.common.Conv                      [1024, 768, 1, 1]             \n",
      " 13                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 14           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
      " 15                -1  3   6200832  models.common.C3                        [1536, 768, 3, False]         \n",
      " 16                -1  1    394240  models.common.Conv                      [768, 512, 1, 1]              \n",
      " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 18           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 19                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 20                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 21                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 22           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 24                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 25          [-1, 20]  1         0  models.common.Concat                    [1]                           \n",
      " 26                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 27                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 28          [-1, 16]  1         0  models.common.Concat                    [1]                           \n",
      " 29                -1  3   5807616  models.common.C3                        [1024, 768, 3, False]         \n",
      " 30                -1  1   5309952  models.common.Conv                      [768, 768, 3, 2]              \n",
      " 31          [-1, 12]  1         0  models.common.Concat                    [1]                           \n",
      " 32                -1  3  10496000  models.common.C3                        [1536, 1024, 3, False]        \n",
      " 33  [23, 26, 29, 32]  1     46152  models.yolo.Detect                      [1, [[19, 27, 44, 40, 38, 94], [96, 68, 86, 152, 180, 137], [140, 301, 303, 264, 238, 542], [436, 615, 739, 380, 925, 792]], [256, 512, 768, 1024]]\n",
      "Model Summary: 607 layers, 76162504 parameters, 76162504 gradients, 110.2 GFLOPs\n",
      "\n",
      "Transferred 787/795 items from yolov5l6.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 131 weight (no decay), 135 weight, 135 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/app/_data/train_37_seq_id_rand0_f2' images and labels...4388 f\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /app/_data/train_37_seq_id_rand0_f2.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/app/_data/val_37_seq_id_rand0_f2' images and labels...531 found,\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /app/_data/val_37_seq_id_rand0_f2.cache\n",
      "Plotting labels to runs/train/yolov5l6_2880_seq_id_rand0_f2/labels.jpg... \n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.65 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset âœ…\n",
      "Image sizes 2880 train, 2880 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/yolov5l6_2880_seq_id_rand0_f2\u001b[0m\n",
      "Starting training for 80 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      0/79     22.9G   0.06567   0.08962         0        11      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.271      0.534      0.434      0.317\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      1/79     22.4G   0.04663   0.06408         0        12      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.707       0.64      0.703      0.528\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      2/79     22.4G   0.04219   0.05716         0        13      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.523      0.568      0.567      0.428\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      3/79     22.4G   0.03872   0.05789         0         6      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.635      0.741      0.736      0.586\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      4/79     22.4G    0.0351     0.053         0         7      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.658      0.788      0.778      0.593\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      5/79     22.4G   0.03378   0.05043         0        14      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.675      0.741      0.748      0.592\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      6/79     22.4G    0.0325   0.05019         0         7      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.622       0.75      0.744      0.598\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      7/79     22.4G   0.03195   0.04922         0        12      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.689      0.691        0.7      0.571\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      8/79     22.4G   0.03061   0.04643         0        24      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.581      0.728      0.692      0.578\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "      9/79     22.4G   0.03022   0.04645         0        16      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.614      0.741      0.699      0.564\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     10/79     22.4G   0.02977   0.04715         0        10      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.635      0.704      0.676      0.569\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     11/79     22.4G   0.02909    0.0447         0        12      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.651      0.747      0.728      0.598\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     12/79     22.4G   0.02888   0.04418         0        11      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.646      0.712      0.671      0.566\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     13/79     22.4G   0.02798   0.04394         0        14      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.612      0.756      0.678      0.554\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "     14/79     22.4G   0.02792   0.04281         0        14      2880: 100%|â–ˆâ–ˆâ–ˆ\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.606      0.707      0.661      0.547\n",
      "Stopping training early as no improvement observed in last 10 epochs. Best results observed at epoch 4, best model saved as best.pt.\n",
      "To update EarlyStopping(patience=10) pass a new patience value, i.e. `python train.py --patience 300` or use `--patience 0` to disable EarlyStopping.\n",
      "\n",
      "15 epochs completed in 15.893 hours.\n",
      "Optimizer stripped from runs/train/yolov5l6_2880_seq_id_rand0_f2/weights/last.pt, 155.3MB\n",
      "Optimizer stripped from runs/train/yolov5l6_2880_seq_id_rand0_f2/weights/best.pt, 155.3MB\n",
      "\n",
      "Validating runs/train/yolov5l6_2880_seq_id_rand0_f2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       2556       1130      0.659      0.788      0.778      0.594\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1672... (success).\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             metrics/F2 â–â–†â–„â–‡â–ˆâ–‡â–‡â–†â–‡â–‡â–†â–‡â–‡â–‡â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 â–â–†â–„â–‡â–ˆâ–‡â–‡â–†â–†â–†â–†â–‡â–†â–†â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 â–â–†â–„â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–‡â–‡â–‡â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision â–â–ˆâ–…â–‡â–‡â–‡â–‡â–ˆâ–†â–‡â–‡â–‡â–‡â–†â–†â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall â–â–„â–‚â–‡â–ˆâ–‡â–‡â–…â–†â–‡â–†â–‡â–†â–‡â–†â–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss â–ˆâ–„â–„â–ƒâ–‚â–‚â–‚â–‚â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss â–ˆâ–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss â–ˆâ–†â–…â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–â–‚â–â–â–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss â–ˆâ–‚â–„â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 â–â–…â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 â–ˆâ–…â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                best/F2 0.75839\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             best/epoch 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           best/mAP_0.5 0.77769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      best/mAP_0.5:0.95 0.59311\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         best/precision 0.6579\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            best/recall 0.7885\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             metrics/F2 0.75872\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.7778\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.59376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.65948\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.78838\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.02792\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.04281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.0064\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.00638\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00943\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 48 media file(s), 1 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33myolov5l6_2880_seq_id_rand0_f2\u001b[0m: \u001b[34mhttps://wandb.ai/tatanko/YOLOv5/runs/38rbf4kx\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220206_075349-38rbf4kx/logs/debug.log\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "Results saved to \u001b[1mruns/train/yolov5l6_2880_seq_id_rand0_f2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img {IMG_SIZE} \\\n",
    "                --batch 2\\\n",
    "                --epochs 80 \\\n",
    "                --data {data_yaml_path} \\\n",
    "                --weights {WEIGHTS} \\\n",
    "                --name {NAME} \\\n",
    "                --hyp data/hyps/hyp.custom.seq.yaml \\\n",
    "                --single-cls \\\n",
    "                --patience 10 \\\n",
    "                --workers 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a297b52-2a28-4252-8867-878c35e07f86",
   "metadata": {},
   "source": [
    "# f2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d28d391-afe4-46f3-bfcc-621202362a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp_fn(gt, prediction, conf_thr):\n",
    "    ious = np.arange(0.3, 0.81, 0.05)\n",
    "    TP, FP, FN = (\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "        np.zeros(ious.shape[0], \"int16\"),\n",
    "    )\n",
    "    prediction = prediction[prediction[:, 4] > conf_thr]\n",
    "    bboxes = prediction[:, :4].astype(\"int\")\n",
    "    bboxes[:, 0] = bboxes[:, 0] - bboxes[:, 2] / 2\n",
    "    bboxes[:, 1] = bboxes[:, 1] - bboxes[:, 3] / 2\n",
    "    bboxes[:, 2] = bboxes[:, 0] + bboxes[:, 2]\n",
    "    bboxes[:, 3] = bboxes[:, 1] + bboxes[:, 3]\n",
    "    if bboxes.size != 0:\n",
    "        if gt.size == 0:\n",
    "            fp = bboxes.shape[0]\n",
    "            FP = np.full(ious.shape[0], fp, \"int16\")\n",
    "        else:\n",
    "            iou_matrix = box_iou(torch.Tensor(gt), torch.Tensor(bboxes))\n",
    "            for n, iou_thr in enumerate(ious):\n",
    "                x = torch.where(iou_matrix >= iou_thr)\n",
    "                tp = np.unique(x[0]).shape[0]\n",
    "                fp = bboxes.shape[0] - tp\n",
    "                fn = gt.shape[0] - tp\n",
    "                TP[n] = tp\n",
    "                FP[n] = fp\n",
    "                FN[n] = fn\n",
    "    else:\n",
    "        if gt.size != 0:\n",
    "            fn = gt.shape[0]\n",
    "            FN = np.full(ious.shape[0], fn, \"int16\")\n",
    "    return TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67d2bbf3-2782-44d4-a1eb-8c0b2e64f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"r\") as f:\n",
    "    res_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b73f0fcf-10b2-467f-8868-6c39df0e4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.0-193-gdb1f83b torch 1.9.1+cu111 CUDA:0 (NVIDIA GeForce RTX 3090, 24268MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 476 layers, 76118664 parameters, 0 gradients, 110.0 GFLOPs\n",
      "Adding AutoShape... \n",
      "100% 2556/2556 [10:15<00:00,  4.15it/s]\n"
     ]
    }
   ],
   "source": [
    "conf_thres = np.arange(0.1, 0.61, 0.01)\n",
    "ious = np.arange(0.3, 0.81, 0.05)\n",
    "res = np.zeros([conf_thres.shape[0], 3, ious.shape[0]])\n",
    "\n",
    "path = f\"/app/_data/yolov5_f2/runs/train/{NAME}/weights/best.pt\"\n",
    "IMG_SIZE = IMG_SIZE\n",
    "model = torch.hub.load(\n",
    "    \"/app/_data/yolov5\", \"custom\", path=path, source=\"local\", force_reload=True\n",
    ")\n",
    "model.conf = 0.01\n",
    "# chose validation set\n",
    "df_test = val.copy()\n",
    "# computing f2 score\n",
    "for ix in tqdm(df_test.index.tolist()):\n",
    "    img = np.array(Image.open(df_test.loc[ix, \"img_path\"]))\n",
    "    prediction = model(img, size=IMG_SIZE, augment=True).xywh[0].cpu().numpy()\n",
    "    prediction = prediction[prediction[:, 4] > 0.1]\n",
    "    gt = np.array([list(x.values()) for x in df_test.loc[ix, \"annotations\"]])\n",
    "    if gt.size:\n",
    "        gt[:, 2] = gt[:, 2] + gt[:, 0]\n",
    "        gt[:, 3] = gt[:, 3] + gt[:, 1]\n",
    "    for n, c_th in enumerate(conf_thres):\n",
    "        TP, FP, FN = tp_fp_fn(gt, prediction, c_th)\n",
    "        res[n, 0, :] += TP\n",
    "        res[n, 1, :] += FP\n",
    "        res[n, 2, :] += FN\n",
    "F2 = np.zeros(conf_thres.shape[0])\n",
    "for c in range(conf_thres.shape[0]):\n",
    "    TP = res[c, 0, :]\n",
    "    FP = res[c, 1, :]\n",
    "    FN = res[c, 2, :]\n",
    "    recall = TP / (TP + FN)\n",
    "    precission = TP / (TP + FP)\n",
    "    f2 = 5 * precission * recall / (4 * precission + recall + 1e-16)\n",
    "    F2[c] = np.mean(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0cbbf96-a064-4d5e-b705-78c1f88c9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if path not in res_dict:\n",
    "    res_dict[path] = {\n",
    "        IMG_SIZE: {\n",
    "            \"best\": [\n",
    "                np.round(conf_thres[np.argmax(F2)], 2),\n",
    "                np.round(np.max(F2), 4),\n",
    "            ],\n",
    "            \"all\": list(np.round(F2, 4)),\n",
    "        }\n",
    "    }\n",
    "else:\n",
    "    res_dict[path][IMG_SIZE] = {\n",
    "        \"best\": [\n",
    "            np.round(conf_thres[np.argmax(F2)], 2),\n",
    "            np.round(np.max(F2), 4),\n",
    "        ],\n",
    "        \"all\": list(np.round(F2, 4)),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63e43699-6710-4a99-8b35-8643069a2846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2880': {'best': [0.12, 0.631],\n",
       "  'all': [0.6298,\n",
       "   0.6287,\n",
       "   0.631,\n",
       "   0.6289,\n",
       "   0.6297,\n",
       "   0.6288,\n",
       "   0.628,\n",
       "   0.6266,\n",
       "   0.6249,\n",
       "   0.6231,\n",
       "   0.624,\n",
       "   0.6242,\n",
       "   0.6235,\n",
       "   0.6211,\n",
       "   0.6204,\n",
       "   0.6149,\n",
       "   0.6114,\n",
       "   0.6102,\n",
       "   0.6094,\n",
       "   0.6063,\n",
       "   0.6064,\n",
       "   0.6032,\n",
       "   0.6003,\n",
       "   0.5969,\n",
       "   0.592,\n",
       "   0.5919,\n",
       "   0.5902,\n",
       "   0.5864,\n",
       "   0.5824,\n",
       "   0.5789,\n",
       "   0.5763,\n",
       "   0.5726,\n",
       "   0.5666,\n",
       "   0.5633,\n",
       "   0.5583,\n",
       "   0.5507,\n",
       "   0.5438,\n",
       "   0.5402,\n",
       "   0.5362,\n",
       "   0.5308,\n",
       "   0.5193,\n",
       "   0.5113,\n",
       "   0.5037,\n",
       "   0.4997,\n",
       "   0.4934,\n",
       "   0.4877,\n",
       "   0.4784,\n",
       "   0.4717,\n",
       "   0.4661,\n",
       "   0.4586,\n",
       "   0.4463]}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_dict['/app/_data/yolov5/runs/train/yolov5l6_2880_seq_id_6/weights/best.pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "606d6f14-e047-4409-a07c-1faf5dbc9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/f2_results.json\", \"w\") as f:\n",
    "    json.dump(res_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3863a-8913-45aa-85d7-38fffb879fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
